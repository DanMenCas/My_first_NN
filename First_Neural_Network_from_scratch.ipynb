{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4Djh9STtX5Zk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bScGmUOLdwTQ",
        "outputId": "4da799b8-2721-4bd1-d741-8dd571709ab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "boqK1lybeTSK"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Neural networks from scratch/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the csv file that contains iris dataset"
      ],
      "metadata": {
        "id": "qXEqhd-zr9SD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SeJMAydRb-Jd"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('iris/iris.data', names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 's_class'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "DIg4V4ohfUwt",
        "outputId": "672de922-7efc-4bf0-d462-4d840585c184"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     sepal_length  sepal_width  petal_length  petal_width         s_class\n",
              "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
              "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
              "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
              "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
              "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
              "..            ...          ...           ...          ...             ...\n",
              "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
              "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
              "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
              "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
              "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
              "\n",
              "[150 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-82200eb5-5502-4b65-b795-63139768a41f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>s_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows Ã— 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82200eb5-5502-4b65-b795-63139768a41f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-82200eb5-5502-4b65-b795-63139768a41f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-82200eb5-5502-4b65-b795-63139768a41f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-98fbde4e-6f14-428e-b16e-f3fcb870ca49\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-98fbde4e-6f14-428e-b16e-f3fcb870ca49')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-98fbde4e-6f14-428e-b16e-f3fcb870ca49 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new column called class that contains only numeric observations for each class"
      ],
      "metadata": {
        "id": "rbWGP2T9sgP3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VVIMpfCOkCDj"
      },
      "outputs": [],
      "source": [
        "n_class = {'Iris-setosa': 1, 'Iris-versicolor': 2, 'Iris-virginica': 3}\n",
        "data['class'] = data['s_class'].replace(n_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "t0lyWgkPgevK"
      },
      "outputs": [],
      "source": [
        "df = data[['sepal_length',\t'sepal_width',\t'petal_length',\t'petal_width',\t'class']].values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are all functions used in this notebook"
      ],
      "metadata": {
        "id": "wT8NNYIXs5mK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "2_xBAHnVlIbA"
      },
      "outputs": [],
      "source": [
        "def split_train_test(df, size, class_index):\n",
        "\n",
        "  n = np.round(df.shape[0]*size).astype('int')\n",
        "\n",
        "  np.random.shuffle(df)\n",
        "\n",
        "  X_train = df[:n, :class_index]\n",
        "\n",
        "  X_test = df[n:, :class_index]\n",
        "\n",
        "  y_train = df[:n, class_index]\n",
        "\n",
        "  y_test = df[n:, class_index]\n",
        "\n",
        "  print(f'Train dataset have {X_train.shape[0]} rows and test have {X_test.shape[0]} rows.')\n",
        "\n",
        "  return X_train, X_test, y_train, y_test\n",
        "\n",
        "# For the Weights we use gorot normal\n",
        "\n",
        "def gorot_normal(n_weights, fan_in, fan_out, seed):\n",
        "\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  limit = np.sqrt(6 / (fan_in + fan_out))\n",
        "\n",
        "  random_weights = np.random.uniform(-limit, limit, n_weights)\n",
        "\n",
        "  random_weights[:, 0] = np.abs(random_weights[:, 0])\n",
        "\n",
        "  return random_weights\n",
        "\n",
        "# Activation function ReLu\n",
        "\n",
        "def ReLu(x):\n",
        "\n",
        "  x = np.where(x < 0, 0, x)\n",
        "\n",
        "  return x\n",
        "\n",
        "# Derivative of activation function ReLu\n",
        "\n",
        "def ReLu_Derivative(x):\n",
        "\n",
        "  x = np.where(x <= 0, 0, 1)\n",
        "\n",
        "  return x\n",
        "\n",
        "# Softmax function\n",
        "\n",
        "def softmax(x):\n",
        "\n",
        "  denom = np.sum(np.exp(x), axis=1).reshape(-1, 1)\n",
        "\n",
        "  r = np.multiply(np.exp(x), 1/denom)\n",
        "\n",
        "  return r\n",
        "\n",
        "def categorical_cross_entropy(x, y_one_hot):\n",
        "\n",
        "  category = np.multiply(x, y_one_hot)\n",
        "\n",
        "  j = -np.log(np.sum(category, axis=1).reshape(1, -1))\n",
        "\n",
        "  return np.sum(j)\n",
        "\n",
        "def first_nn_training(X, y, lr, weights1, weights2):\n",
        "\n",
        "  w1 = weights1\n",
        "\n",
        "  w2 = weights2\n",
        "\n",
        "  diff_w1 = []\n",
        "\n",
        "  diff_w2 = []\n",
        "\n",
        "  accuracy = []\n",
        "\n",
        "  p_break = 1\n",
        "\n",
        "  i = 1\n",
        "\n",
        "  while (p_break > 1e-04):\n",
        "\n",
        "    # We obtain the outputs of respective layers\n",
        "\n",
        "    # FORWARD PROPAGATION\n",
        "\n",
        "    # First layer with ReLu activation\n",
        "\n",
        "    a_1 = ReLu(np.dot(X, w1))\n",
        "\n",
        "    #Second Layer with softmax activation\n",
        "\n",
        "    a_2 = softmax(np.dot(a_1, w2))\n",
        "\n",
        "    #We obtain the loss function, in this case categorical corssentropy\n",
        "\n",
        "    cce = categorical_cross_entropy(a_2, y)\n",
        "\n",
        "    #Accuracy\n",
        "\n",
        "    diff = np.argmax(a_2, axis = 1)-np.argmax(y, axis = 1)\n",
        "    acc = diff[diff == 0].shape[0]/diff.shape[0]\n",
        "\n",
        "    # First calculate of the gradients, this is the product between the\n",
        "    # derivative of the loss function and the softmax activation (the first and)\n",
        "    # and second derivative of the chain.\n",
        "\n",
        "    # BACKPROAGATION AND UPDATED WEIGHTS\n",
        "\n",
        "    g = a_2 - y\n",
        "\n",
        "    # Next we calculate the weights gradient, this will be the input for update\n",
        "    # the weights\n",
        "\n",
        "    # This varible help us to compare the difference between weights and\n",
        "    # configure a limit for stop the iterations.\n",
        "\n",
        "    pre_w2 = w2\n",
        "\n",
        "    # weights 2 gradient\n",
        "\n",
        "    w2_g = np.dot(a_1.T, g)\n",
        "\n",
        "    # Update the weights\n",
        "\n",
        "    w2 = w2 - lr*w2_g\n",
        "\n",
        "    # Calculate again the gradient for propagate for the others layers\n",
        "\n",
        "    g = np.dot(g, pre_w2.T)\n",
        "\n",
        "    # Gradient of the hidden layer\n",
        "\n",
        "    g = np.multiply(ReLu_Derivative(a_1), g)\n",
        "\n",
        "    pre_w1 = w1\n",
        "\n",
        "    # weights 1 gradient\n",
        "\n",
        "    w1_g = np.dot(X_train.T, g)\n",
        "\n",
        "    # Update the weights\n",
        "\n",
        "    w1 = w1 - lr*w1_g\n",
        "\n",
        "    print(f'Accuracy in step {i + 1} is {acc}')\n",
        "\n",
        "    # In here we calculate variables for stop the process\n",
        "\n",
        "    diff_w1.append(np.average(np.abs(pre_w1 - w1)))\n",
        "\n",
        "    diff_w2.append(np.average(np.abs(pre_w2 - w2)))\n",
        "\n",
        "    p_break = np.average(np.abs(pre_w1 - w1))\n",
        "\n",
        "    accuracy.append(acc)\n",
        "\n",
        "    print(f'Average difference between weights for second layer {np.average(np.abs(pre_w2 - w2))} and fisrt layer {np.average(np.abs(pre_w1 - w1))}')\n",
        "\n",
        "    i += 1\n",
        "\n",
        "  return w1, w2, diff_w1, diff_w2, accuracy\n",
        "\n",
        "def first_nn_test(X, y, weights1, weights2):\n",
        "\n",
        "  # this function is to apply the weights for the test dataset\n",
        "\n",
        "  a_1 = ReLu(np.dot(X, w1))\n",
        "\n",
        "  a_2 = softmax(np.dot(a_1, w2))\n",
        "\n",
        "  diff = np.argmax(a_2, axis = 1)-np.argmax(y, axis = 1)\n",
        "  acc = diff[diff == 0].shape[0]/diff.shape[0]\n",
        "\n",
        "  pred = np.argmax(a_2, axis = 1)+1\n",
        "\n",
        "  print(f'Accuracy in test set is {acc*100}%')\n",
        "\n",
        "  return pred\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take train and test data sets from the iris dataset"
      ],
      "metadata": {
        "id": "YIfkP7qgtAY4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpAAQdWLokJw",
        "outputId": "a766aedf-8476-40e5-a2b9-2db811e650c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset have 120 rows and test have 30 rows.\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = split_train_test(df, 0.8, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Truth values to one-hot encoding"
      ],
      "metadata": {
        "id": "AnpH0P-htqC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# y to one-hot\n",
        "\n",
        "unique_values = np.unique(y_train)\n",
        "\n",
        "y_train_one_hot = (y_train[:, None] == unique_values).astype(int)\n",
        "\n",
        "unique_values = np.unique(y_test)\n",
        "\n",
        "y_test_one_hot = (y_test[:, None] == unique_values).astype(int)"
      ],
      "metadata": {
        "id": "QzfwpICt8zUX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mFYy_wG6pwLm"
      },
      "outputs": [],
      "source": [
        "# Neural network will have 2 hidden layers, first with 20 nodes and second with 10\n",
        "\n",
        "nodes_first = 20\n",
        "nodes_second = 3\n",
        "seed = 123456\n",
        "lr = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HFnjOSICgNA5"
      },
      "outputs": [],
      "source": [
        "# First add the ones column to the df for the bias term.\n",
        "\n",
        "ones = np.ones((X_train.shape[0], 1))\n",
        "\n",
        "X_train = np.hstack((ones, X_train))\n",
        "\n",
        "ones = np.ones((X_test.shape[0], 1))\n",
        "\n",
        "X_test = np.hstack((ones, X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "vnLVisAH6YxG"
      },
      "outputs": [],
      "source": [
        "# Calculate the random weights.\n",
        "\n",
        "weights_1 = gorot_normal((X_train.shape[1], nodes_first+1), nodes_first, nodes_second, seed)\n",
        "\n",
        "weights_2 = gorot_normal((nodes_first+1, nodes_second), nodes_first, nodes_second, seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1, w2, diff_w1, diff_w2, accuracy = first_nn_training(X_train, y_train_one_hot, 0.0001, weights_1, weights_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k92_wgVShhuT",
        "outputId": "8ddc2c5a-ba98-4449-d5c2-feefb6c87f88"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy in step 2 is 0.325\n",
            "Average difference between weights for second layer 0.005095071382842002 and fisrt layer 0.0037746370420361475\n",
            "Accuracy in step 3 is 0.325\n",
            "Average difference between weights for second layer 0.00449678268832031 and fisrt layer 0.003344502398210344\n",
            "Accuracy in step 4 is 0.325\n",
            "Average difference between weights for second layer 0.003840704211882642 and fisrt layer 0.0027529790317429603\n",
            "Accuracy in step 5 is 0.325\n",
            "Average difference between weights for second layer 0.00315380192399175 and fisrt layer 0.0020628128024309965\n",
            "Accuracy in step 6 is 0.325\n",
            "Average difference between weights for second layer 0.0025146271263517457 and fisrt layer 0.0014214415418952544\n",
            "Accuracy in step 7 is 0.325\n",
            "Average difference between weights for second layer 0.0020409508128601535 and fisrt layer 0.0010232728182878112\n",
            "Accuracy in step 8 is 0.325\n",
            "Average difference between weights for second layer 0.0016980142427788644 and fisrt layer 0.0008492990775266255\n",
            "Accuracy in step 9 is 0.325\n",
            "Average difference between weights for second layer 0.0014518198066728427 and fisrt layer 0.0007350534811664702\n",
            "Accuracy in step 10 is 0.325\n",
            "Average difference between weights for second layer 0.0012785976229542802 and fisrt layer 0.0006558724686655761\n",
            "Accuracy in step 11 is 0.425\n",
            "Average difference between weights for second layer 0.0011448474781674597 and fisrt layer 0.0006378025513700317\n",
            "Accuracy in step 12 is 0.5333333333333333\n",
            "Average difference between weights for second layer 0.0010364671960419898 and fisrt layer 0.0006230821681545397\n",
            "Accuracy in step 13 is 0.6416666666666667\n",
            "Average difference between weights for second layer 0.0009499255140305034 and fisrt layer 0.00062126833878865\n",
            "Accuracy in step 14 is 0.7\n",
            "Average difference between weights for second layer 0.0008766960118317155 and fisrt layer 0.0006015442619755851\n",
            "Accuracy in step 15 is 0.7666666666666667\n",
            "Average difference between weights for second layer 0.0008132170857275059 and fisrt layer 0.0005827450904771043\n",
            "Accuracy in step 16 is 0.8666666666666667\n",
            "Average difference between weights for second layer 0.0007599270728169142 and fisrt layer 0.0005430035193142329\n",
            "Accuracy in step 17 is 0.9\n",
            "Average difference between weights for second layer 0.0007135062567304422 and fisrt layer 0.0005023655579895711\n",
            "Accuracy in step 18 is 0.9333333333333333\n",
            "Average difference between weights for second layer 0.000673817479425537 and fisrt layer 0.0004578548753065006\n",
            "Accuracy in step 19 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.000638210334168328 and fisrt layer 0.0004247623372203428\n",
            "Accuracy in step 20 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.0006081416012739789 and fisrt layer 0.0003843993573360474\n",
            "Accuracy in step 21 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0005827793984955743 and fisrt layer 0.000355191340661481\n",
            "Accuracy in step 22 is 0.975\n",
            "Average difference between weights for second layer 0.0005615116095292539 and fisrt layer 0.0003259742305759229\n",
            "Accuracy in step 23 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0005437920422148004 and fisrt layer 0.00029587458464090617\n",
            "Accuracy in step 24 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0005292497789221093 and fisrt layer 0.00028713115341297523\n",
            "Accuracy in step 25 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0005169541453184272 and fisrt layer 0.00028379356078266164\n",
            "Accuracy in step 26 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0005054321171950561 and fisrt layer 0.0002759023641627512\n",
            "Accuracy in step 27 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0004957522431712899 and fisrt layer 0.00026780109370973097\n",
            "Accuracy in step 28 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00048708931229236375 and fisrt layer 0.00026104005476135895\n",
            "Accuracy in step 29 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0004794040363397178 and fisrt layer 0.00025337778750471714\n",
            "Accuracy in step 30 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00047245333749151885 and fisrt layer 0.0002480031859085208\n",
            "Accuracy in step 31 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0004662176818956241 and fisrt layer 0.00024398682028920726\n",
            "Accuracy in step 32 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00046003596644332903 and fisrt layer 0.0002429218717653\n",
            "Accuracy in step 33 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0004541632676352474 and fisrt layer 0.00024059750181912415\n",
            "Accuracy in step 34 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0004486622962602957 and fisrt layer 0.00024090825856345726\n",
            "Accuracy in step 35 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0004434592654128661 and fisrt layer 0.00024183722312008343\n",
            "Accuracy in step 36 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00043875946749246194 and fisrt layer 0.00023895439910131983\n",
            "Accuracy in step 37 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00043380587119784547 and fisrt layer 0.00024077620482308796\n",
            "Accuracy in step 38 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0004292320157731259 and fisrt layer 0.00023914758583483214\n",
            "Accuracy in step 39 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.000424865097936468 and fisrt layer 0.00023634658513717346\n",
            "Accuracy in step 40 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00042050615135833257 and fisrt layer 0.0002360135360744211\n",
            "Accuracy in step 41 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0004164866369575065 and fisrt layer 0.00023645267329778472\n",
            "Accuracy in step 42 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00041279680003842217 and fisrt layer 0.00023579873616420567\n",
            "Accuracy in step 43 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0004091845938260343 and fisrt layer 0.0002378448163038861\n",
            "Accuracy in step 44 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00040607335197163485 and fisrt layer 0.00024050621575557815\n",
            "Accuracy in step 45 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0004028936180403133 and fisrt layer 0.00024373653003775206\n",
            "Accuracy in step 46 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0004002669051667757 and fisrt layer 0.00025240905419691593\n",
            "Accuracy in step 47 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00039755989402483807 and fisrt layer 0.0002619620246062196\n",
            "Accuracy in step 48 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00039547887832572134 and fisrt layer 0.00026124317195976563\n",
            "Accuracy in step 49 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00039345575487086424 and fisrt layer 0.00026630035397046445\n",
            "Accuracy in step 50 is 0.9583333333333334\n",
            "Average difference between weights for second layer 0.0003920889019396267 and fisrt layer 0.0002568555532024439\n",
            "Accuracy in step 51 is 0.9583333333333334\n",
            "Average difference between weights for second layer 0.00039060601432728734 and fisrt layer 0.0002527510587161866\n",
            "Accuracy in step 52 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0003891050576864369 and fisrt layer 0.00024249822967759103\n",
            "Accuracy in step 53 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00038736496379771597 and fisrt layer 0.00023379264311663845\n",
            "Accuracy in step 54 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.000385630874222068 and fisrt layer 0.0002082285025198996\n",
            "Accuracy in step 55 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0003826481636353187 and fisrt layer 0.0002104995368071632\n",
            "Accuracy in step 56 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0003797186338399724 and fisrt layer 0.0002010931782450048\n",
            "Accuracy in step 57 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00037646955479596046 and fisrt layer 0.0002003252138083054\n",
            "Accuracy in step 58 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0003733629716713 and fisrt layer 0.0001967679967762248\n",
            "Accuracy in step 59 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00037007246125718164 and fisrt layer 0.00019432233327331585\n",
            "Accuracy in step 60 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00036672116751255946 and fisrt layer 0.00019418705441732196\n",
            "Accuracy in step 61 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0003635989810356715 and fisrt layer 0.00019134676110889044\n",
            "Accuracy in step 62 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0003603894772457001 and fisrt layer 0.00019194039880285956\n",
            "Accuracy in step 63 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00035744426369131206 and fisrt layer 0.0001898461674017593\n",
            "Accuracy in step 64 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0003544708873885194 and fisrt layer 0.00018903661914795034\n",
            "Accuracy in step 65 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00035161210954187373 and fisrt layer 0.00018760738420570505\n",
            "Accuracy in step 66 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0003486707176528035 and fisrt layer 0.00018911768219048857\n",
            "Accuracy in step 67 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00034569383951550014 and fisrt layer 0.0001857732053619532\n",
            "Accuracy in step 68 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0003428544639628339 and fisrt layer 0.00018960097338225587\n",
            "Accuracy in step 69 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0003400175197102858 and fisrt layer 0.00018392261708506208\n",
            "Accuracy in step 70 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0003374634919431158 and fisrt layer 0.00018585073044846836\n",
            "Accuracy in step 71 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00033440959454149837 and fisrt layer 0.0001826225332657749\n",
            "Accuracy in step 72 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0003319559723793465 and fisrt layer 0.000184405985841666\n",
            "Accuracy in step 73 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00032908055291674807 and fisrt layer 0.00018317371187827355\n",
            "Accuracy in step 74 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0003262396945717065 and fisrt layer 0.00018050118516945267\n",
            "Accuracy in step 75 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00032396612146508985 and fisrt layer 0.0001818566964922102\n",
            "Accuracy in step 76 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.000321264504643984 and fisrt layer 0.0001808528734022161\n",
            "Accuracy in step 77 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0003186643955998394 and fisrt layer 0.00018113597091749098\n",
            "Accuracy in step 78 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0003162624989486262 and fisrt layer 0.00017841381763398421\n",
            "Accuracy in step 79 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0003137388850277231 and fisrt layer 0.00017777750486734184\n",
            "Accuracy in step 80 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00031128834004005054 and fisrt layer 0.00017609738063552422\n",
            "Accuracy in step 81 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00030935689897622695 and fisrt layer 0.00017643865296887117\n",
            "Accuracy in step 82 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00030702335745573614 and fisrt layer 0.00017688261944679315\n",
            "Accuracy in step 83 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0003049150181713001 and fisrt layer 0.00017551143538778086\n",
            "Accuracy in step 84 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00030280233786159495 and fisrt layer 0.00017252660456459838\n",
            "Accuracy in step 85 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00030047639907921096 and fisrt layer 0.00017437473275261635\n",
            "Accuracy in step 86 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002984751151597048 and fisrt layer 0.00017368623119082906\n",
            "Accuracy in step 87 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00029650836153911176 and fisrt layer 0.0001723989419507197\n",
            "Accuracy in step 88 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00029452781908636644 and fisrt layer 0.00017146743120056155\n",
            "Accuracy in step 89 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002925284585008953 and fisrt layer 0.00016987179687643172\n",
            "Accuracy in step 90 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00029044743233167116 and fisrt layer 0.0001706097362119101\n",
            "Accuracy in step 91 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002885154377371875 and fisrt layer 0.00017002078357197564\n",
            "Accuracy in step 92 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00028661806868087103 and fisrt layer 0.00017048488120332558\n",
            "Accuracy in step 93 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002845606514911648 and fisrt layer 0.00016849086296889096\n",
            "Accuracy in step 94 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002827751080719146 and fisrt layer 0.0001689818614041903\n",
            "Accuracy in step 95 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00028082182313902276 and fisrt layer 0.0001679587128938729\n",
            "Accuracy in step 96 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002792796502981349 and fisrt layer 0.00016743341670766575\n",
            "Accuracy in step 97 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002773840238649043 and fisrt layer 0.0001665700450792879\n",
            "Accuracy in step 98 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00027555339086810004 and fisrt layer 0.00016503094550273528\n",
            "Accuracy in step 99 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00027378390994419784 and fisrt layer 0.00016641866277993482\n",
            "Accuracy in step 100 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002722920396320928 and fisrt layer 0.0001634368883093042\n",
            "Accuracy in step 101 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002705671709425368 and fisrt layer 0.00016483993394992766\n",
            "Accuracy in step 102 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002691217836257295 and fisrt layer 0.0001618952072034498\n",
            "Accuracy in step 103 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00026744064690685033 and fisrt layer 0.00016330941282394318\n",
            "Accuracy in step 104 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002660362065931145 and fisrt layer 0.0001603973186206966\n",
            "Accuracy in step 105 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00026439906469154706 and fisrt layer 0.00016302860871884823\n",
            "Accuracy in step 106 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.000263131950047721 and fisrt layer 0.0001577118373821913\n",
            "Accuracy in step 107 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002614284887353729 and fisrt layer 0.00016156589249513532\n",
            "Accuracy in step 108 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00026019573443442876 and fisrt layer 0.00015728407236163867\n",
            "Accuracy in step 109 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002586408249073988 and fisrt layer 0.00016009878580924993\n",
            "Accuracy in step 110 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00025742718618358544 and fisrt layer 0.00015594114631231807\n",
            "Accuracy in step 111 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002558982874653589 and fisrt layer 0.0001586628373333162\n",
            "Accuracy in step 112 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00025470555454504257 and fisrt layer 0.00015563502037569597\n",
            "Accuracy in step 113 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002532991441417365 and fisrt layer 0.00015604835127762913\n",
            "Accuracy in step 114 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002520274414940464 and fisrt layer 0.00015425775494622282\n",
            "Accuracy in step 115 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002506529671011938 and fisrt layer 0.00015568850849065582\n",
            "Accuracy in step 116 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00024951020196092486 and fisrt layer 0.00015198320027765268\n",
            "Accuracy in step 117 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002479768161411488 and fisrt layer 0.00015422128558145113\n",
            "Accuracy in step 118 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00024679191285677303 and fisrt layer 0.00015477315910020426\n",
            "Accuracy in step 119 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002451307220572775 and fisrt layer 0.0001514743243177681\n",
            "Accuracy in step 120 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00024393628590980865 and fisrt layer 0.00015223766190055525\n",
            "Accuracy in step 121 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00024253624139083187 and fisrt layer 0.0001544647153470518\n",
            "Accuracy in step 122 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0002411867612666577 and fisrt layer 0.00014827446456782404\n",
            "Accuracy in step 123 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00023978294086052115 and fisrt layer 0.00015359481125435474\n",
            "Accuracy in step 124 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00023848333994909147 and fisrt layer 0.0001487645433136834\n",
            "Accuracy in step 125 is 0.975\n",
            "Average difference between weights for second layer 0.00023691491650359983 and fisrt layer 0.00015238902916899573\n",
            "Accuracy in step 126 is 0.975\n",
            "Average difference between weights for second layer 0.00023596586508643522 and fisrt layer 0.00014926310743389935\n",
            "Accuracy in step 127 is 0.975\n",
            "Average difference between weights for second layer 0.0002342107330065921 and fisrt layer 0.0001499044739325382\n",
            "Accuracy in step 128 is 0.975\n",
            "Average difference between weights for second layer 0.00023324155079147834 and fisrt layer 0.00014718451956868538\n",
            "Accuracy in step 129 is 0.975\n",
            "Average difference between weights for second layer 0.0002317465839743848 and fisrt layer 0.00015124141258412882\n",
            "Accuracy in step 130 is 0.975\n",
            "Average difference between weights for second layer 0.0002306801426934371 and fisrt layer 0.00014580801253791847\n",
            "Accuracy in step 131 is 0.975\n",
            "Average difference between weights for second layer 0.00022923209750265363 and fisrt layer 0.0001494757106355993\n",
            "Accuracy in step 132 is 0.975\n",
            "Average difference between weights for second layer 0.00022811969543787836 and fisrt layer 0.00014595388264529972\n",
            "Accuracy in step 133 is 0.975\n",
            "Average difference between weights for second layer 0.00022682554040646114 and fisrt layer 0.00014829257207740946\n",
            "Accuracy in step 134 is 0.975\n",
            "Average difference between weights for second layer 0.00022574276938289146 and fisrt layer 0.0001461848148649617\n",
            "Accuracy in step 135 is 0.975\n",
            "Average difference between weights for second layer 0.00022431303675036782 and fisrt layer 0.00014538011006444553\n",
            "Accuracy in step 136 is 0.975\n",
            "Average difference between weights for second layer 0.00022347149956286792 and fisrt layer 0.00014298702543309524\n",
            "Accuracy in step 137 is 0.975\n",
            "Average difference between weights for second layer 0.00022199840943242153 and fisrt layer 0.00014658180349738797\n",
            "Accuracy in step 138 is 0.975\n",
            "Average difference between weights for second layer 0.00022108175408259344 and fisrt layer 0.00014152770776640664\n",
            "Accuracy in step 139 is 0.975\n",
            "Average difference between weights for second layer 0.00021966416138161836 and fisrt layer 0.00014463835889340728\n",
            "Accuracy in step 140 is 0.975\n",
            "Average difference between weights for second layer 0.0002189059282835211 and fisrt layer 0.00014121667190405298\n",
            "Accuracy in step 141 is 0.975\n",
            "Average difference between weights for second layer 0.0002175622480747429 and fisrt layer 0.00014402351103101946\n",
            "Accuracy in step 142 is 0.975\n",
            "Average difference between weights for second layer 0.00021662776743293653 and fisrt layer 0.0001402042404240552\n",
            "Accuracy in step 143 is 0.975\n",
            "Average difference between weights for second layer 0.00021527737071007258 and fisrt layer 0.00014306919443139078\n",
            "Accuracy in step 144 is 0.975\n",
            "Average difference between weights for second layer 0.0002144559874205868 and fisrt layer 0.00013887259717568288\n",
            "Accuracy in step 145 is 0.975\n",
            "Average difference between weights for second layer 0.00021315709114219067 and fisrt layer 0.0001423548822367937\n",
            "Accuracy in step 146 is 0.975\n",
            "Average difference between weights for second layer 0.0002122906061783291 and fisrt layer 0.00013815217810342917\n",
            "Accuracy in step 147 is 0.975\n",
            "Average difference between weights for second layer 0.00021119190656429933 and fisrt layer 0.0001391865125806387\n",
            "Accuracy in step 148 is 0.975\n",
            "Average difference between weights for second layer 0.0002102359719355149 and fisrt layer 0.00013981145406359477\n",
            "Accuracy in step 149 is 0.975\n",
            "Average difference between weights for second layer 0.00020931207040911546 and fisrt layer 0.00013667124422842963\n",
            "Accuracy in step 150 is 0.975\n",
            "Average difference between weights for second layer 0.00020824802610724694 and fisrt layer 0.00013747482788679653\n",
            "Accuracy in step 151 is 0.975\n",
            "Average difference between weights for second layer 0.00020726797608020233 and fisrt layer 0.00013845482541511892\n",
            "Accuracy in step 152 is 0.975\n",
            "Average difference between weights for second layer 0.00020637123865663203 and fisrt layer 0.00013532225309450282\n",
            "Accuracy in step 153 is 0.975\n",
            "Average difference between weights for second layer 0.00020534405929822973 and fisrt layer 0.00013817655577056997\n",
            "Accuracy in step 154 is 0.975\n",
            "Average difference between weights for second layer 0.00020429127226680277 and fisrt layer 0.00013539160174118122\n",
            "Accuracy in step 155 is 0.975\n",
            "Average difference between weights for second layer 0.0002033853971929161 and fisrt layer 0.0001353732602612533\n",
            "Accuracy in step 156 is 0.975\n",
            "Average difference between weights for second layer 0.00020247644708586032 and fisrt layer 0.00013660032623671355\n",
            "Accuracy in step 157 is 0.975\n",
            "Average difference between weights for second layer 0.0002014768624132094 and fisrt layer 0.00013392370759997385\n",
            "Accuracy in step 158 is 0.975\n",
            "Average difference between weights for second layer 0.0002006139184180354 and fisrt layer 0.00013467843751081545\n",
            "Accuracy in step 159 is 0.975\n",
            "Average difference between weights for second layer 0.0001995629937037083 and fisrt layer 0.00013441905982319578\n",
            "Accuracy in step 160 is 0.975\n",
            "Average difference between weights for second layer 0.00019879909320021883 and fisrt layer 0.0001346720784258578\n",
            "Accuracy in step 161 is 0.975\n",
            "Average difference between weights for second layer 0.0001978546097414201 and fisrt layer 0.00013208802191430098\n",
            "Accuracy in step 162 is 0.975\n",
            "Average difference between weights for second layer 0.0001970406453355614 and fisrt layer 0.0001343526157592852\n",
            "Accuracy in step 163 is 0.975\n",
            "Average difference between weights for second layer 0.00019616170722212465 and fisrt layer 0.00013093748209866738\n",
            "Accuracy in step 164 is 0.975\n",
            "Average difference between weights for second layer 0.00019532305519755178 and fisrt layer 0.0001330408204910764\n",
            "Accuracy in step 165 is 0.975\n",
            "Average difference between weights for second layer 0.00019441803064563975 and fisrt layer 0.00013138337713200043\n",
            "Accuracy in step 166 is 0.975\n",
            "Average difference between weights for second layer 0.0001936991704589119 and fisrt layer 0.00013268492191553194\n",
            "Accuracy in step 167 is 0.975\n",
            "Average difference between weights for second layer 0.00019286941041579532 and fisrt layer 0.00013138005993337782\n",
            "Accuracy in step 168 is 0.975\n",
            "Average difference between weights for second layer 0.00019203098752803514 and fisrt layer 0.00012988248559734155\n",
            "Accuracy in step 169 is 0.975\n",
            "Average difference between weights for second layer 0.00019124886451306062 and fisrt layer 0.00013090793261799925\n",
            "Accuracy in step 170 is 0.975\n",
            "Average difference between weights for second layer 0.00019040148544667711 and fisrt layer 0.00013071498972865478\n",
            "Accuracy in step 171 is 0.975\n",
            "Average difference between weights for second layer 0.00018970301345092033 and fisrt layer 0.00012961007964065217\n",
            "Accuracy in step 172 is 0.975\n",
            "Average difference between weights for second layer 0.000188884806116563 and fisrt layer 0.00012819452364187862\n",
            "Accuracy in step 173 is 0.975\n",
            "Average difference between weights for second layer 0.00018806595360309534 and fisrt layer 0.00012927033815493042\n",
            "Accuracy in step 174 is 0.975\n",
            "Average difference between weights for second layer 0.00018730773497156402 and fisrt layer 0.00012874459634560237\n",
            "Accuracy in step 175 is 0.975\n",
            "Average difference between weights for second layer 0.00018654531828329665 and fisrt layer 0.00012957244998701103\n",
            "Accuracy in step 176 is 0.975\n",
            "Average difference between weights for second layer 0.00018590779492214004 and fisrt layer 0.00012763236370801642\n",
            "Accuracy in step 177 is 0.975\n",
            "Average difference between weights for second layer 0.00018512344246849312 and fisrt layer 0.00012732067565384545\n",
            "Accuracy in step 178 is 0.975\n",
            "Average difference between weights for second layer 0.0001843552792335772 and fisrt layer 0.00012790551003181712\n",
            "Accuracy in step 179 is 0.975\n",
            "Average difference between weights for second layer 0.0001836312635562939 and fisrt layer 0.00012698472427706488\n",
            "Accuracy in step 180 is 0.975\n",
            "Average difference between weights for second layer 0.0001829756121239642 and fisrt layer 0.0001260982073451282\n",
            "Accuracy in step 181 is 0.975\n",
            "Average difference between weights for second layer 0.00018221401216544626 and fisrt layer 0.00012673479092030026\n",
            "Accuracy in step 182 is 0.975\n",
            "Average difference between weights for second layer 0.00018150100943382103 and fisrt layer 0.00012549709443475038\n",
            "Accuracy in step 183 is 0.975\n",
            "Average difference between weights for second layer 0.00018077178718693752 and fisrt layer 0.00012730468065394078\n",
            "Accuracy in step 184 is 0.975\n",
            "Average difference between weights for second layer 0.00018020098094570375 and fisrt layer 0.0001246138098951511\n",
            "Accuracy in step 185 is 0.975\n",
            "Average difference between weights for second layer 0.00017948903864315794 and fisrt layer 0.00012435617711293673\n",
            "Accuracy in step 186 is 0.975\n",
            "Average difference between weights for second layer 0.00017883005267637029 and fisrt layer 0.0001249672143916656\n",
            "Accuracy in step 187 is 0.975\n",
            "Average difference between weights for second layer 0.0001782313017182875 and fisrt layer 0.0001241055247496419\n",
            "Accuracy in step 188 is 0.975\n",
            "Average difference between weights for second layer 0.00017775982232903152 and fisrt layer 0.00012500113163574854\n",
            "Accuracy in step 189 is 0.975\n",
            "Average difference between weights for second layer 0.0001771665459825673 and fisrt layer 0.00012296322798964577\n",
            "Accuracy in step 190 is 0.975\n",
            "Average difference between weights for second layer 0.00017653417846841957 and fisrt layer 0.00012269180821021614\n",
            "Accuracy in step 191 is 0.975\n",
            "Average difference between weights for second layer 0.00017591881015176726 and fisrt layer 0.00012413581247402728\n",
            "Accuracy in step 192 is 0.975\n",
            "Average difference between weights for second layer 0.0001753837170661658 and fisrt layer 0.0001224156602454327\n",
            "Accuracy in step 193 is 0.975\n",
            "Average difference between weights for second layer 0.0001749381574137217 and fisrt layer 0.0001232996727243427\n",
            "Accuracy in step 194 is 0.975\n",
            "Average difference between weights for second layer 0.00017436773493823837 and fisrt layer 0.00012130166151847319\n",
            "Accuracy in step 195 is 0.975\n",
            "Average difference between weights for second layer 0.00017375844254163356 and fisrt layer 0.0001229481914321865\n",
            "Accuracy in step 196 is 0.975\n",
            "Average difference between weights for second layer 0.00017323367610837617 and fisrt layer 0.0001215531795274091\n",
            "Accuracy in step 197 is 0.975\n",
            "Average difference between weights for second layer 0.00017268769964990532 and fisrt layer 0.00012159438881678456\n",
            "Accuracy in step 198 is 0.975\n",
            "Average difference between weights for second layer 0.00017229193532747626 and fisrt layer 0.0001207524719163619\n",
            "Accuracy in step 199 is 0.975\n",
            "Average difference between weights for second layer 0.00017170567605013452 and fisrt layer 0.00012157069000837013\n",
            "Accuracy in step 200 is 0.975\n",
            "Average difference between weights for second layer 0.00017118146413742688 and fisrt layer 0.00012021783365038362\n",
            "Accuracy in step 201 is 0.975\n",
            "Average difference between weights for second layer 0.00017064086411294665 and fisrt layer 0.00011993040920984746\n",
            "Accuracy in step 202 is 0.975\n",
            "Average difference between weights for second layer 0.0001701086897825861 and fisrt layer 0.00012068767795131778\n",
            "Accuracy in step 203 is 0.975\n",
            "Average difference between weights for second layer 0.00016961807073393608 and fisrt layer 0.00012052026546603828\n",
            "Accuracy in step 204 is 0.975\n",
            "Average difference between weights for second layer 0.0001692751096575429 and fisrt layer 0.00011800131617784959\n",
            "Accuracy in step 205 is 0.975\n",
            "Average difference between weights for second layer 0.00016867903433476623 and fisrt layer 0.00012050904144244646\n",
            "Accuracy in step 206 is 0.975\n",
            "Average difference between weights for second layer 0.00016820241665059648 and fisrt layer 0.000118321598697138\n",
            "Accuracy in step 207 is 0.975\n",
            "Average difference between weights for second layer 0.00016768048021918283 and fisrt layer 0.0001180532034681674\n",
            "Accuracy in step 208 is 0.975\n",
            "Average difference between weights for second layer 0.00016716555000448826 and fisrt layer 0.00011916290825218583\n",
            "Accuracy in step 209 is 0.975\n",
            "Average difference between weights for second layer 0.0001668305564238269 and fisrt layer 0.00011814633809962007\n",
            "Accuracy in step 210 is 0.975\n",
            "Average difference between weights for second layer 0.00016630989298660508 and fisrt layer 0.00011810364049521783\n",
            "Accuracy in step 211 is 0.975\n",
            "Average difference between weights for second layer 0.00016581734361378886 and fisrt layer 0.00011681754519374437\n",
            "Accuracy in step 212 is 0.975\n",
            "Average difference between weights for second layer 0.00016530771847333452 and fisrt layer 0.0001173960063472963\n",
            "Accuracy in step 213 is 0.975\n",
            "Average difference between weights for second layer 0.0001648361576126131 and fisrt layer 0.00011730420868709626\n",
            "Accuracy in step 214 is 0.975\n",
            "Average difference between weights for second layer 0.000164374061695682 and fisrt layer 0.00011600294624564459\n",
            "Accuracy in step 215 is 0.975\n",
            "Average difference between weights for second layer 0.00016388594350480115 and fisrt layer 0.00011792034823653733\n",
            "Accuracy in step 216 is 0.975\n",
            "Average difference between weights for second layer 0.0001635990321914114 and fisrt layer 0.00011633235428285091\n",
            "Accuracy in step 217 is 0.975\n",
            "Average difference between weights for second layer 0.00016321723902357535 and fisrt layer 0.00011662068375242632\n",
            "Accuracy in step 218 is 0.975\n",
            "Average difference between weights for second layer 0.00016268502218226764 and fisrt layer 0.00011573147001071975\n",
            "Accuracy in step 219 is 0.975\n",
            "Average difference between weights for second layer 0.00016229746679993624 and fisrt layer 0.00011603719530550383\n",
            "Accuracy in step 220 is 0.975\n",
            "Average difference between weights for second layer 0.0001617646322330393 and fisrt layer 0.00011551274625222786\n",
            "Accuracy in step 221 is 0.975\n",
            "Average difference between weights for second layer 0.00016151607651936943 and fisrt layer 0.00011530680764328824\n",
            "Accuracy in step 222 is 0.975\n",
            "Average difference between weights for second layer 0.00016093730497187194 and fisrt layer 0.00011452964820466068\n",
            "Accuracy in step 223 is 0.975\n",
            "Average difference between weights for second layer 0.00016053145873429951 and fisrt layer 0.00011487826375514515\n",
            "Accuracy in step 224 is 0.975\n",
            "Average difference between weights for second layer 0.00015999193204167255 and fisrt layer 0.00011440344170793212\n",
            "Accuracy in step 225 is 0.975\n",
            "Average difference between weights for second layer 0.00015974592329509734 and fisrt layer 0.0001142095926502521\n",
            "Accuracy in step 226 is 0.975\n",
            "Average difference between weights for second layer 0.0001591724167002792 and fisrt layer 0.00011426179004359467\n",
            "Accuracy in step 227 is 0.975\n",
            "Average difference between weights for second layer 0.00015880493770880334 and fisrt layer 0.00011299304706896294\n",
            "Accuracy in step 228 is 0.975\n",
            "Average difference between weights for second layer 0.00015824794496873022 and fisrt layer 0.00011381463603116143\n",
            "Accuracy in step 229 is 0.975\n",
            "Average difference between weights for second layer 0.00015790537038862246 and fisrt layer 0.00011365038957650419\n",
            "Accuracy in step 230 is 0.975\n",
            "Average difference between weights for second layer 0.00015752436195165783 and fisrt layer 0.00011325805325713256\n",
            "Accuracy in step 231 is 0.975\n",
            "Average difference between weights for second layer 0.00015714430412316013 and fisrt layer 0.00011272143347093908\n",
            "Accuracy in step 232 is 0.975\n",
            "Average difference between weights for second layer 0.000156730756723182 and fisrt layer 0.00011234296880442227\n",
            "Accuracy in step 233 is 0.975\n",
            "Average difference between weights for second layer 0.0001561818856005144 and fisrt layer 0.00011224317192228986\n",
            "Accuracy in step 234 is 0.975\n",
            "Average difference between weights for second layer 0.00015577332248450468 and fisrt layer 0.00011261114696824379\n",
            "Accuracy in step 235 is 0.975\n",
            "Average difference between weights for second layer 0.00015553438965610968 and fisrt layer 0.00011132951748940244\n",
            "Accuracy in step 236 is 0.975\n",
            "Average difference between weights for second layer 0.00015489986203598544 and fisrt layer 0.00011152742891547718\n",
            "Accuracy in step 237 is 0.975\n",
            "Average difference between weights for second layer 0.0001545018328074732 and fisrt layer 0.00011233606168567986\n",
            "Accuracy in step 238 is 0.975\n",
            "Average difference between weights for second layer 0.00015416854927213886 and fisrt layer 0.00011076634421314378\n",
            "Accuracy in step 239 is 0.975\n",
            "Average difference between weights for second layer 0.0001536519159420228 and fisrt layer 0.00011122850327418257\n",
            "Accuracy in step 240 is 0.975\n",
            "Average difference between weights for second layer 0.00015336197973698444 and fisrt layer 0.00011132043260336861\n",
            "Accuracy in step 241 is 0.975\n",
            "Average difference between weights for second layer 0.00015293994597231683 and fisrt layer 0.00011022200609064424\n",
            "Accuracy in step 242 is 0.975\n",
            "Average difference between weights for second layer 0.0001525014182179863 and fisrt layer 0.00011012377510299911\n",
            "Accuracy in step 243 is 0.975\n",
            "Average difference between weights for second layer 0.0001520666370960102 and fisrt layer 0.00011047466793437942\n",
            "Accuracy in step 244 is 0.975\n",
            "Average difference between weights for second layer 0.00015169186055941352 and fisrt layer 0.00010915882795210225\n",
            "Accuracy in step 245 is 0.975\n",
            "Average difference between weights for second layer 0.0001513665055665903 and fisrt layer 0.00010961019912412552\n",
            "Accuracy in step 246 is 0.975\n",
            "Average difference between weights for second layer 0.00015094867756722483 and fisrt layer 0.00010985445363535162\n",
            "Accuracy in step 247 is 0.975\n",
            "Average difference between weights for second layer 0.00015061337250186005 and fisrt layer 0.00010938228569922759\n",
            "Accuracy in step 248 is 0.975\n",
            "Average difference between weights for second layer 0.00015019977118205764 and fisrt layer 0.00010723126247482943\n",
            "Accuracy in step 249 is 0.975\n",
            "Average difference between weights for second layer 0.00014985179216073306 and fisrt layer 0.00010928133476515088\n",
            "Accuracy in step 250 is 0.975\n",
            "Average difference between weights for second layer 0.00014944507844273785 and fisrt layer 0.00010893049340906319\n",
            "Accuracy in step 251 is 0.975\n",
            "Average difference between weights for second layer 0.00014905852463956073 and fisrt layer 0.00010864063029883166\n",
            "Accuracy in step 252 is 0.975\n",
            "Average difference between weights for second layer 0.0001486846065680616 and fisrt layer 0.00010779351706737944\n",
            "Accuracy in step 253 is 0.975\n",
            "Average difference between weights for second layer 0.00014832762777308963 and fisrt layer 0.00010693572449407457\n",
            "Accuracy in step 254 is 0.975\n",
            "Average difference between weights for second layer 0.00014803761273971187 and fisrt layer 0.00010809522415464785\n",
            "Accuracy in step 255 is 0.975\n",
            "Average difference between weights for second layer 0.00014764989102430292 and fisrt layer 0.00010686868575076429\n",
            "Accuracy in step 256 is 0.975\n",
            "Average difference between weights for second layer 0.00014724119146432621 and fisrt layer 0.0001075344978856095\n",
            "Accuracy in step 257 is 0.975\n",
            "Average difference between weights for second layer 0.00014688446932184374 and fisrt layer 0.0001072979448981359\n",
            "Accuracy in step 258 is 0.975\n",
            "Average difference between weights for second layer 0.00014653298925415393 and fisrt layer 0.00010606472883289849\n",
            "Accuracy in step 259 is 0.975\n",
            "Average difference between weights for second layer 0.0001463027281246118 and fisrt layer 0.00010624437556975183\n",
            "Accuracy in step 260 is 0.975\n",
            "Average difference between weights for second layer 0.00014587730903909722 and fisrt layer 0.00010704880750026454\n",
            "Accuracy in step 261 is 0.975\n",
            "Average difference between weights for second layer 0.00014552972530749644 and fisrt layer 0.00010628873223026227\n",
            "Accuracy in step 262 is 0.975\n",
            "Average difference between weights for second layer 0.00014520783891129677 and fisrt layer 0.00010612268761070471\n",
            "Accuracy in step 263 is 0.975\n",
            "Average difference between weights for second layer 0.00014489363477203867 and fisrt layer 0.00010593830752411738\n",
            "Accuracy in step 264 is 0.975\n",
            "Average difference between weights for second layer 0.00014458449237189018 and fisrt layer 0.0001048578363275323\n",
            "Accuracy in step 265 is 0.975\n",
            "Average difference between weights for second layer 0.00014425444964601543 and fisrt layer 0.000104574102655145\n",
            "Accuracy in step 266 is 0.975\n",
            "Average difference between weights for second layer 0.0001439221626024115 and fisrt layer 0.00010638226692892127\n",
            "Accuracy in step 267 is 0.975\n",
            "Average difference between weights for second layer 0.00014364267023615667 and fisrt layer 0.00010527281297510805\n",
            "Accuracy in step 268 is 0.975\n",
            "Average difference between weights for second layer 0.0001433642872278357 and fisrt layer 0.00010412260508560283\n",
            "Accuracy in step 269 is 0.975\n",
            "Average difference between weights for second layer 0.0001430537480741138 and fisrt layer 0.00010549702215394789\n",
            "Accuracy in step 270 is 0.975\n",
            "Average difference between weights for second layer 0.00014278163821692636 and fisrt layer 0.00010455664139385901\n",
            "Accuracy in step 271 is 0.975\n",
            "Average difference between weights for second layer 0.0001424902753367168 and fisrt layer 0.00010434915028783258\n",
            "Accuracy in step 272 is 0.975\n",
            "Average difference between weights for second layer 0.00014219902255805053 and fisrt layer 0.00010516804777123488\n",
            "Accuracy in step 273 is 0.975\n",
            "Average difference between weights for second layer 0.00014198623588470593 and fisrt layer 0.00010376517124423629\n",
            "Accuracy in step 274 is 0.975\n",
            "Average difference between weights for second layer 0.000141674913689332 and fisrt layer 0.00010347840161470418\n",
            "Accuracy in step 275 is 0.975\n",
            "Average difference between weights for second layer 0.00014137010883541446 and fisrt layer 0.00010349071411453338\n",
            "Accuracy in step 276 is 0.975\n",
            "Average difference between weights for second layer 0.00014107570432578804 and fisrt layer 0.00010234153725204754\n",
            "Accuracy in step 277 is 0.975\n",
            "Average difference between weights for second layer 0.00014071924820157968 and fisrt layer 0.00010413305431112457\n",
            "Accuracy in step 278 is 0.975\n",
            "Average difference between weights for second layer 0.00014049111742533182 and fisrt layer 0.00010309415028332607\n",
            "Accuracy in step 279 is 0.975\n",
            "Average difference between weights for second layer 0.00014022740779853215 and fisrt layer 0.00010352626872849938\n",
            "Accuracy in step 280 is 0.975\n",
            "Average difference between weights for second layer 0.0001399727937650783 and fisrt layer 0.00010174301864388877\n",
            "Accuracy in step 281 is 0.975\n",
            "Average difference between weights for second layer 0.00013967390322396035 and fisrt layer 0.00010309824126126506\n",
            "Accuracy in step 282 is 0.975\n",
            "Average difference between weights for second layer 0.00013941317881700622 and fisrt layer 0.0001022034556983504\n",
            "Accuracy in step 283 is 0.975\n",
            "Average difference between weights for second layer 0.00013913543061450619 and fisrt layer 0.00010270153213468856\n",
            "Accuracy in step 284 is 0.975\n",
            "Average difference between weights for second layer 0.00013887584631763387 and fisrt layer 0.00010178181066076701\n",
            "Accuracy in step 285 is 0.975\n",
            "Average difference between weights for second layer 0.000138598720252693 and fisrt layer 0.00010228463399338982\n",
            "Accuracy in step 286 is 0.975\n",
            "Average difference between weights for second layer 0.00013834016759780538 and fisrt layer 0.00010239640406526992\n",
            "Accuracy in step 287 is 0.975\n",
            "Average difference between weights for second layer 0.0001381395465185096 and fisrt layer 0.00010104539815939497\n",
            "Accuracy in step 288 is 0.975\n",
            "Average difference between weights for second layer 0.00013784460758053856 and fisrt layer 0.00010161784128123588\n",
            "Accuracy in step 289 is 0.975\n",
            "Average difference between weights for second layer 0.00013757787511513495 and fisrt layer 9.999794887234205e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_nn_test(X_test, y_test_one_hot, w1, w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySHwOIlIujo1",
        "outputId": "687251b6-cbf7-4f8a-c7bb-e4c5c2593a8a"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy in test set is 96.66666666666667%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 3, 3, 2, 3, 3, 1, 3, 1, 1, 1, 3, 2, 2, 3, 3, 1, 2, 2, 3, 2, 1,\n",
              "       2, 3, 1, 2, 1, 1, 3, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}