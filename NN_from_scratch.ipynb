{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4Djh9STtX5Zk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bScGmUOLdwTQ",
        "outputId": "add89123-0c04-4ac7-aa20-e588e1738f28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "boqK1lybeTSK"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Neural networks from scratch/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the csv file that contains iris dataset"
      ],
      "metadata": {
        "id": "qXEqhd-zr9SD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SeJMAydRb-Jd"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('iris/iris.data', names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 's_class'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "DIg4V4ohfUwt",
        "outputId": "a76f6e71-aaaf-45df-a111-0f91b908a767"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     sepal_length  sepal_width  petal_length  petal_width         s_class\n",
              "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
              "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
              "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
              "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
              "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
              "..            ...          ...           ...          ...             ...\n",
              "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
              "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
              "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
              "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
              "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
              "\n",
              "[150 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dc1ad0a6-343a-449b-82c5-130db7de5eda\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>s_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows Ã— 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc1ad0a6-343a-449b-82c5-130db7de5eda')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dc1ad0a6-343a-449b-82c5-130db7de5eda button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dc1ad0a6-343a-449b-82c5-130db7de5eda');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-851c4d2c-b996-482f-9f37-0e6d40f91924\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-851c4d2c-b996-482f-9f37-0e6d40f91924')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-851c4d2c-b996-482f-9f37-0e6d40f91924 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_8ec19924-637c-487c-9d8e-ee793c35d0bb\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8ec19924-637c-487c-9d8e-ee793c35d0bb button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 150,\n  \"fields\": [\n    {\n      \"column\": \"sepal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8280661279778629,\n        \"min\": 4.3,\n        \"max\": 7.9,\n        \"num_unique_values\": 35,\n        \"samples\": [\n          6.2,\n          4.5,\n          5.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sepal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4335943113621737,\n        \"min\": 2.0,\n        \"max\": 4.4,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          2.3,\n          4.0,\n          3.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7644204199522617,\n        \"min\": 1.0,\n        \"max\": 6.9,\n        \"num_unique_values\": 43,\n        \"samples\": [\n          6.7,\n          3.8,\n          3.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7631607417008414,\n        \"min\": 0.1,\n        \"max\": 2.5,\n        \"num_unique_values\": 22,\n        \"samples\": [\n          0.2,\n          1.2,\n          1.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"s_class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Iris-setosa\",\n          \"Iris-versicolor\",\n          \"Iris-virginica\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new column called class that contains only numeric observations for each class"
      ],
      "metadata": {
        "id": "rbWGP2T9sgP3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VVIMpfCOkCDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02083a80-308a-4109-a277-bc204f1e0497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3397770899.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  data['class'] = data['s_class'].replace(n_class)\n"
          ]
        }
      ],
      "source": [
        "n_class = {'Iris-setosa': 1, 'Iris-versicolor': 2, 'Iris-virginica': 3}\n",
        "data['class'] = data['s_class'].replace(n_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t0lyWgkPgevK"
      },
      "outputs": [],
      "source": [
        "df = data[['sepal_length',\t'sepal_width',\t'petal_length',\t'petal_width',\t'class']].values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are all functions used in this notebook"
      ],
      "metadata": {
        "id": "wT8NNYIXs5mK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2_xBAHnVlIbA"
      },
      "outputs": [],
      "source": [
        "#Function to split the data into train ans test set\n",
        "\n",
        "def split_train_test(df, size, class_index):\n",
        "\n",
        "  n = np.round(df.shape[0]*size).astype('int')\n",
        "\n",
        "  np.random.shuffle(df)\n",
        "\n",
        "  X_train = df[:n, :class_index]\n",
        "\n",
        "  X_test = df[n:, :class_index]\n",
        "\n",
        "  y_train = df[:n, class_index]\n",
        "\n",
        "  y_test = df[n:, class_index]\n",
        "\n",
        "  print(f'Train dataset have {X_train.shape[0]} rows and test have {X_test.shape[0]} rows.')\n",
        "\n",
        "  return X_train, X_test, y_train, y_test\n",
        "\n",
        "# For the Weights we use gorot normal\n",
        "\n",
        "def gorot_normal(n_weights, fan_in, fan_out, seed):\n",
        "\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  limit = np.sqrt(6 / (fan_in + fan_out))\n",
        "\n",
        "  random_weights = np.random.uniform(-limit, limit, n_weights)\n",
        "\n",
        "  random_weights[:, 0] = np.abs(random_weights[:, 0])\n",
        "\n",
        "  return random_weights\n",
        "\n",
        "# Activation function ReLu\n",
        "\n",
        "def ReLu(x):\n",
        "\n",
        "  x = np.where(x < 0, 0, x)\n",
        "\n",
        "  return x\n",
        "\n",
        "# Derivative of activation function ReLu\n",
        "\n",
        "def relu_derivative(x):\n",
        "\n",
        "  x = np.where(x <= 0, 0, 1)\n",
        "\n",
        "  return x\n",
        "\n",
        "# Softmax function\n",
        "\n",
        "def softmax(x):\n",
        "\n",
        "  denom = np.sum(np.exp(x), axis=1).reshape(-1, 1)\n",
        "\n",
        "  r = np.multiply(np.exp(x), 1/denom)\n",
        "\n",
        "  return r\n",
        "\n",
        "def categorical_cross_entropy(y_pred, y_one_hot):\n",
        "\n",
        "  category = np.multiply(y_pred, y_one_hot)\n",
        "\n",
        "  j = -np.log(np.sum(category, axis=1).reshape(1, -1))\n",
        "\n",
        "  return np.sum(j)\n",
        "\n",
        "def first_nn_training(X, y, lr, weights1, weights2):\n",
        "\n",
        "  w1 = weights1\n",
        "\n",
        "  w2 = weights2\n",
        "\n",
        "  diff_w1 = []\n",
        "\n",
        "  diff_w2 = []\n",
        "\n",
        "  accuracy = []\n",
        "\n",
        "  p_break = 1\n",
        "\n",
        "  i = 1\n",
        "\n",
        "  while (p_break > 1e-04):\n",
        "\n",
        "    # We obtain the outputs of the respective layers\n",
        "\n",
        "    # FORWARD PROPAGATION\n",
        "\n",
        "    # First layer with ReLu activation, it means we multiply X values by the\n",
        "    # weights and the result pass through a ReLu.\n",
        "\n",
        "    a_1 = ReLu(np.dot(X, w1))\n",
        "\n",
        "    # Same process as first layer but instead of use X, we are using the result\n",
        "    # of the first layer multiplied by the respective weights\n",
        "\n",
        "    a_2 = softmax(np.dot(a_1, w2))\n",
        "\n",
        "    # We obtain the loss function, in this case categorical corssentropy\n",
        "\n",
        "    cce = categorical_cross_entropy(a_2, y)\n",
        "\n",
        "    # Accuracy\n",
        "\n",
        "    diff = np.argmax(a_2, axis = 1)-np.argmax(y, axis = 1)\n",
        "    acc = diff[diff == 0].shape[0]/diff.shape[0]\n",
        "\n",
        "    # Calculation of the gradients, this is the product between the\n",
        "    # derivative of the loss function and the softmax activation in the output\n",
        "    # layer\n",
        "\n",
        "    # BACKPROAGATION AND UPDATED WEIGHTS\n",
        "\n",
        "    g = a_2 - y\n",
        "\n",
        "    # This variable help us to compare the difference between weights and\n",
        "    # configure a limit for stop the iterations.\n",
        "\n",
        "    pre_w2 = w2\n",
        "\n",
        "    # Next we calculate the weights gradient, this will be the input for update\n",
        "    # the weights and it is simply multiply the gradient by the output of the\n",
        "    # previous layer following the chain rule of differentiation.\n",
        "\n",
        "    # Output layer weights gradient\n",
        "\n",
        "    w2_g = np.dot(a_1.T, g)\n",
        "\n",
        "    # The learning rate (lr) help us to control the influence of the gradient in the\n",
        "    # updating process of the weights.\n",
        "\n",
        "    w2 = w2 - lr*w2_g\n",
        "\n",
        "    # Calculate again the gradient for propagate for the others layers,\n",
        "    # again following the chain rule of differentiation we just multiply the\n",
        "    # gradient by the original weights of the output layer.\n",
        "\n",
        "    g = np.dot(g, pre_w2.T)\n",
        "\n",
        "    # Gradient of the hidden layer\n",
        "\n",
        "    g = np.multiply(relu_derivative(a_1), g)\n",
        "\n",
        "    pre_w1 = w1\n",
        "\n",
        "    # weights 1 gradient the last step in the chain rule in this\n",
        "    # specific neural network\n",
        "\n",
        "    w1_g = np.dot(X_train.T, g)\n",
        "\n",
        "    # Update the weights\n",
        "\n",
        "    w1 = w1 - lr*w1_g\n",
        "\n",
        "    print(f'Accuracy in step {i + 1} is {acc}')\n",
        "\n",
        "    # In here we calculate variables to stop the process\n",
        "\n",
        "    diff_w1.append(np.average(np.abs(pre_w1 - w1)))\n",
        "\n",
        "    diff_w2.append(np.average(np.abs(pre_w2 - w2)))\n",
        "\n",
        "    p_break = (np.average(np.abs(pre_w1 - w1))+np.average(np.abs(pre_w2 - w2)))/2\n",
        "\n",
        "    accuracy.append(acc)\n",
        "\n",
        "    print(f'Average difference between weights for second layer {np.average(np.abs(pre_w2 - w2))} and fisrt layer {np.average(np.abs(pre_w1 - w1))}')\n",
        "\n",
        "    i += 1\n",
        "\n",
        "  return w1, w2, diff_w1, diff_w2, accuracy\n",
        "\n",
        "def first_nn_test(X, y, weights1, weights2):\n",
        "\n",
        "  # this function apply the updated weights for the test dataset\n",
        "\n",
        "  a_1 = ReLu(np.dot(X, w1))\n",
        "\n",
        "  a_2 = softmax(np.dot(a_1, w2))\n",
        "\n",
        "  diff = np.argmax(a_2, axis = 1)-np.argmax(y, axis = 1)\n",
        "  acc = diff[diff == 0].shape[0]/diff.shape[0]\n",
        "\n",
        "  pred = np.argmax(a_2, axis = 1)+1\n",
        "\n",
        "  print(f'Accuracy in test set is {acc*100}%')\n",
        "\n",
        "  return pred\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take train and test data sets from the iris dataset"
      ],
      "metadata": {
        "id": "YIfkP7qgtAY4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpAAQdWLokJw",
        "outputId": "3f1183f4-bb93-4715-c521-77fb95ac01e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset have 120 rows and test have 30 rows.\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = split_train_test(df, 0.8, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Truth values to one-hot encoding"
      ],
      "metadata": {
        "id": "AnpH0P-htqC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# y to one-hot\n",
        "\n",
        "unique_values = np.unique(y_train)\n",
        "\n",
        "y_train_one_hot = (y_train[:, None] == unique_values).astype(int)\n",
        "\n",
        "unique_values = np.unique(y_test)\n",
        "\n",
        "y_test_one_hot = (y_test[:, None] == unique_values).astype(int)"
      ],
      "metadata": {
        "id": "QzfwpICt8zUX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mFYy_wG6pwLm"
      },
      "outputs": [],
      "source": [
        "# Neural network will have 1 hidden layer with 20 nodes and output layer with 3 the classes of the dataset\n",
        "\n",
        "nodes_first = 20\n",
        "nodes_second = 3\n",
        "seed = 123456\n",
        "lr = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HFnjOSICgNA5"
      },
      "outputs": [],
      "source": [
        "# First add the ones column to the df for the bias term.\n",
        "\n",
        "ones = np.ones((X_train.shape[0], 1))\n",
        "\n",
        "X_train = np.hstack((ones, X_train))\n",
        "\n",
        "ones = np.ones((X_test.shape[0], 1))\n",
        "\n",
        "X_test = np.hstack((ones, X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vnLVisAH6YxG"
      },
      "outputs": [],
      "source": [
        "# Calculate the random weights.\n",
        "\n",
        "weights_1 = gorot_normal((X_train.shape[1], nodes_first+1), nodes_first, nodes_second, seed)\n",
        "\n",
        "weights_2 = gorot_normal((nodes_first+1, nodes_second), nodes_first, nodes_second, seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1, w2, diff_w1, diff_w2, accuracy = first_nn_training(X_train, y_train_one_hot, lr, weights_1, weights_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k92_wgVShhuT",
        "outputId": "b4feab64-9c4d-40d3-bc23-23d50cb9d340"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy in step 2 is 0.31666666666666665\n",
            "Average difference between weights for second layer 0.005138695208360651 and fisrt layer 0.003838482345149462\n",
            "Accuracy in step 3 is 0.31666666666666665\n",
            "Average difference between weights for second layer 0.004527723623510301 and fisrt layer 0.0034031193060633084\n",
            "Accuracy in step 4 is 0.31666666666666665\n",
            "Average difference between weights for second layer 0.0038548958795456027 and fisrt layer 0.002811818331818252\n",
            "Accuracy in step 5 is 0.31666666666666665\n",
            "Average difference between weights for second layer 0.003139648376174368 and fisrt layer 0.0021123420435464326\n",
            "Accuracy in step 6 is 0.31666666666666665\n",
            "Average difference between weights for second layer 0.002486840528827372 and fisrt layer 0.0014390701600807195\n",
            "Accuracy in step 7 is 0.31666666666666665\n",
            "Average difference between weights for second layer 0.0020085837124157553 and fisrt layer 0.0010805649863360042\n",
            "Accuracy in step 8 is 0.31666666666666665\n",
            "Average difference between weights for second layer 0.0016632794359821594 and fisrt layer 0.0008714452260934068\n",
            "Accuracy in step 9 is 0.31666666666666665\n",
            "Average difference between weights for second layer 0.0014223958281665598 and fisrt layer 0.0007989415294570119\n",
            "Accuracy in step 10 is 0.38333333333333336\n",
            "Average difference between weights for second layer 0.0012585178096427114 and fisrt layer 0.0007110157928960262\n",
            "Accuracy in step 11 is 0.55\n",
            "Average difference between weights for second layer 0.001131350798465061 and fisrt layer 0.0006954956569831847\n",
            "Accuracy in step 12 is 0.6416666666666667\n",
            "Average difference between weights for second layer 0.0010279651736761113 and fisrt layer 0.0006592321390243469\n",
            "Accuracy in step 13 is 0.6666666666666666\n",
            "Average difference between weights for second layer 0.0009424866760426146 and fisrt layer 0.0006270520694493224\n",
            "Accuracy in step 14 is 0.7\n",
            "Average difference between weights for second layer 0.0008686088801157277 and fisrt layer 0.0005718391916771202\n",
            "Accuracy in step 15 is 0.7333333333333333\n",
            "Average difference between weights for second layer 0.0008073835504615645 and fisrt layer 0.0005262046597359025\n",
            "Accuracy in step 16 is 0.8\n",
            "Average difference between weights for second layer 0.0007548326459693982 and fisrt layer 0.00048507597031720035\n",
            "Accuracy in step 17 is 0.8666666666666667\n",
            "Average difference between weights for second layer 0.000708820742293337 and fisrt layer 0.00045997978029521246\n",
            "Accuracy in step 18 is 0.8833333333333333\n",
            "Average difference between weights for second layer 0.0006682287341099767 and fisrt layer 0.0003943132714186263\n",
            "Accuracy in step 19 is 0.9083333333333333\n",
            "Average difference between weights for second layer 0.0006352284253028129 and fisrt layer 0.00037791752099028056\n",
            "Accuracy in step 20 is 0.9416666666666667\n",
            "Average difference between weights for second layer 0.0006065363083217549 and fisrt layer 0.000347368152520676\n",
            "Accuracy in step 21 is 0.9416666666666667\n",
            "Average difference between weights for second layer 0.0005815474599075921 and fisrt layer 0.00029207387616242745\n",
            "Accuracy in step 22 is 0.975\n",
            "Average difference between weights for second layer 0.0005625476286051429 and fisrt layer 0.0002830838468536962\n",
            "Accuracy in step 23 is 0.975\n",
            "Average difference between weights for second layer 0.0005451560273982447 and fisrt layer 0.00027527756815647904\n",
            "Accuracy in step 24 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0005302025059943569 and fisrt layer 0.00026969382768853527\n",
            "Accuracy in step 25 is 0.9583333333333334\n",
            "Average difference between weights for second layer 0.0005174099760968966 and fisrt layer 0.0002637000210064155\n",
            "Accuracy in step 26 is 0.9583333333333334\n",
            "Average difference between weights for second layer 0.0005063985679080245 and fisrt layer 0.00026020898868683764\n",
            "Accuracy in step 27 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0004962141368104079 and fisrt layer 0.0002498113342010082\n",
            "Accuracy in step 28 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0004873256759442222 and fisrt layer 0.00024949566423982045\n",
            "Accuracy in step 29 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00047943462223030975 and fisrt layer 0.00024740750587574396\n",
            "Accuracy in step 30 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0004723571384500712 and fisrt layer 0.00024610110650660365\n",
            "Accuracy in step 31 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0004658951344792007 and fisrt layer 0.00024486740662875195\n",
            "Accuracy in step 32 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00046005103579703293 and fisrt layer 0.0002403501906701303\n",
            "Accuracy in step 33 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00045411083850051953 and fisrt layer 0.0002385259852462329\n",
            "Accuracy in step 34 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00044848642544721934 and fisrt layer 0.00023740470111297064\n",
            "Accuracy in step 35 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00044337271924468354 and fisrt layer 0.00023824025345973155\n",
            "Accuracy in step 36 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.0004385951545502174 and fisrt layer 0.00023752059305704823\n",
            "Accuracy in step 37 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00043429228689362883 and fisrt layer 0.00023262729895754327\n",
            "Accuracy in step 38 is 0.9666666666666667\n",
            "Average difference between weights for second layer 0.00042964660973667136 and fisrt layer 0.00023218374434541615\n",
            "Accuracy in step 39 is 0.975\n",
            "Average difference between weights for second layer 0.00042534934553364287 and fisrt layer 0.0002316015717550009\n",
            "Accuracy in step 40 is 0.975\n",
            "Average difference between weights for second layer 0.00042090228355928 and fisrt layer 0.0002287824382175267\n",
            "Accuracy in step 41 is 0.975\n",
            "Average difference between weights for second layer 0.00041654132530218974 and fisrt layer 0.00023104583416248654\n",
            "Accuracy in step 42 is 0.975\n",
            "Average difference between weights for second layer 0.0004128655745691774 and fisrt layer 0.00022631839579601026\n",
            "Accuracy in step 43 is 0.975\n",
            "Average difference between weights for second layer 0.0004089836902425846 and fisrt layer 0.00022931099329830015\n",
            "Accuracy in step 44 is 0.975\n",
            "Average difference between weights for second layer 0.00040562370444314247 and fisrt layer 0.0002273739017714067\n",
            "Accuracy in step 45 is 0.975\n",
            "Average difference between weights for second layer 0.00040187720380464496 and fisrt layer 0.00022930389753366592\n",
            "Accuracy in step 46 is 0.975\n",
            "Average difference between weights for second layer 0.00039846609957831444 and fisrt layer 0.00023415024073086745\n",
            "Accuracy in step 47 is 0.975\n",
            "Average difference between weights for second layer 0.0003950628633248201 and fisrt layer 0.000234932695572247\n",
            "Accuracy in step 48 is 0.975\n",
            "Average difference between weights for second layer 0.00039187423732949927 and fisrt layer 0.0002410468407041176\n",
            "Accuracy in step 49 is 0.975\n",
            "Average difference between weights for second layer 0.0003890716400568854 and fisrt layer 0.0002534000597942518\n",
            "Accuracy in step 50 is 0.975\n",
            "Average difference between weights for second layer 0.00038666370281329615 and fisrt layer 0.000248315874428957\n",
            "Accuracy in step 51 is 0.975\n",
            "Average difference between weights for second layer 0.00038437812961724613 and fisrt layer 0.0002601747780080803\n",
            "Accuracy in step 52 is 0.975\n",
            "Average difference between weights for second layer 0.0003828441398488122 and fisrt layer 0.0002502087327580933\n",
            "Accuracy in step 53 is 0.975\n",
            "Average difference between weights for second layer 0.0003815193928552513 and fisrt layer 0.00024569817066235407\n",
            "Accuracy in step 54 is 0.975\n",
            "Average difference between weights for second layer 0.00038052814732179716 and fisrt layer 0.0002479629854322044\n",
            "Accuracy in step 55 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00037978508769904377 and fisrt layer 0.00022832498063588715\n",
            "Accuracy in step 56 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00037794485064668315 and fisrt layer 0.00022457510648578828\n",
            "Accuracy in step 57 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00037599256243931817 and fisrt layer 0.00020843515747557038\n",
            "Accuracy in step 58 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.0003731293343240756 and fisrt layer 0.00019998444080256264\n",
            "Accuracy in step 59 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.0003696897640961218 and fisrt layer 0.00019841169623652553\n",
            "Accuracy in step 60 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.0003665129443548592 and fisrt layer 0.00019239029899072727\n",
            "Accuracy in step 61 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00036316685500692046 and fisrt layer 0.00019114007642282934\n",
            "Accuracy in step 62 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00035959964902806236 and fisrt layer 0.00018971928076305276\n",
            "Accuracy in step 63 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.0003559144660695648 and fisrt layer 0.00018771931258094933\n",
            "Accuracy in step 64 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00035230735750626645 and fisrt layer 0.00018764054973195146\n",
            "Accuracy in step 65 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00034903503042392595 and fisrt layer 0.0001867058562808455\n",
            "Accuracy in step 66 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.0003458218751043906 and fisrt layer 0.00018426782840272015\n",
            "Accuracy in step 67 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00034296994574345755 and fisrt layer 0.00018525438953181147\n",
            "Accuracy in step 68 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00033994819996626915 and fisrt layer 0.00018468034780121776\n",
            "Accuracy in step 69 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00033704245769860944 and fisrt layer 0.00018404639456217568\n",
            "Accuracy in step 70 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00033424424838164005 and fisrt layer 0.00018437807845418574\n",
            "Accuracy in step 71 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00033165248412408353 and fisrt layer 0.0001810372502790493\n",
            "Accuracy in step 72 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.0003289501304527862 and fisrt layer 0.00018157108724181986\n",
            "Accuracy in step 73 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.0003264314647525628 and fisrt layer 0.00018095377974596054\n",
            "Accuracy in step 74 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00032397072090681783 and fisrt layer 0.00018026341134958046\n",
            "Accuracy in step 75 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00032156761197731976 and fisrt layer 0.00017952688034162984\n",
            "Accuracy in step 76 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00031922029217937883 and fisrt layer 0.00017876132668195012\n",
            "Accuracy in step 77 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00031692614938497307 and fisrt layer 0.00017797778139719536\n",
            "Accuracy in step 78 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00031479605235748743 and fisrt layer 0.00017811807997670235\n",
            "Accuracy in step 79 is 0.9833333333333333\n",
            "Average difference between weights for second layer 0.00031290897327714674 and fisrt layer 0.00017534908344991646\n",
            "Accuracy in step 80 is 0.975\n",
            "Average difference between weights for second layer 0.000310784466263587 and fisrt layer 0.00017650860820721232\n",
            "Accuracy in step 81 is 0.975\n",
            "Average difference between weights for second layer 0.00030897971844903586 and fisrt layer 0.0001756641201017034\n",
            "Accuracy in step 82 is 0.975\n",
            "Average difference between weights for second layer 0.00030719050741676864 and fisrt layer 0.00017382609381530923\n",
            "Accuracy in step 83 is 0.975\n",
            "Average difference between weights for second layer 0.00030528902239085884 and fisrt layer 0.00017400577722369317\n",
            "Accuracy in step 84 is 0.975\n",
            "Average difference between weights for second layer 0.0003035552468245224 and fisrt layer 0.0001731617516866855\n",
            "Accuracy in step 85 is 0.975\n",
            "Average difference between weights for second layer 0.0003018363695103899 and fisrt layer 0.00017133766961673123\n",
            "Accuracy in step 86 is 0.975\n",
            "Average difference between weights for second layer 0.00030000714145506 and fisrt layer 0.00017130454050724316\n",
            "Accuracy in step 87 is 0.975\n",
            "Average difference between weights for second layer 0.0002982510433988257 and fisrt layer 0.00017036012226702708\n",
            "Accuracy in step 88 is 0.975\n",
            "Average difference between weights for second layer 0.0002964357028275522 and fisrt layer 0.00017079340480397195\n",
            "Accuracy in step 89 is 0.975\n",
            "Average difference between weights for second layer 0.0002944004503822017 and fisrt layer 0.00016982732891890275\n",
            "Accuracy in step 90 is 0.975\n",
            "Average difference between weights for second layer 0.00029244114501648715 and fisrt layer 0.0001689449102604841\n",
            "Accuracy in step 91 is 0.975\n",
            "Average difference between weights for second layer 0.00029054273622826794 and fisrt layer 0.0001681145803424848\n",
            "Accuracy in step 92 is 0.975\n",
            "Average difference between weights for second layer 0.00028869512717167033 and fisrt layer 0.00016731736993181405\n",
            "Accuracy in step 93 is 0.975\n",
            "Average difference between weights for second layer 0.00028689140928750845 and fisrt layer 0.00016629342908756095\n",
            "Accuracy in step 94 is 0.975\n",
            "Average difference between weights for second layer 0.00028503612465372095 and fisrt layer 0.00016532981304981072\n",
            "Accuracy in step 95 is 0.975\n",
            "Average difference between weights for second layer 0.0002831263992570483 and fisrt layer 0.00016471382557303982\n",
            "Accuracy in step 96 is 0.975\n",
            "Average difference between weights for second layer 0.0002812624904410956 and fisrt layer 0.00016407644047950753\n",
            "Accuracy in step 97 is 0.975\n",
            "Average difference between weights for second layer 0.0002794472417431945 and fisrt layer 0.00016551571736991573\n",
            "Accuracy in step 98 is 0.975\n",
            "Average difference between weights for second layer 0.0002775500431647529 and fisrt layer 0.00016455110992852332\n",
            "Accuracy in step 99 is 0.975\n",
            "Average difference between weights for second layer 0.00027572914246452136 and fisrt layer 0.00016367723244138066\n",
            "Accuracy in step 100 is 0.975\n",
            "Average difference between weights for second layer 0.00027402786600021054 and fisrt layer 0.00016227797889014774\n",
            "Accuracy in step 101 is 0.975\n",
            "Average difference between weights for second layer 0.00027219749392045554 and fisrt layer 0.00016107145482413047\n",
            "Accuracy in step 102 is 0.975\n",
            "Average difference between weights for second layer 0.0002704500093879075 and fisrt layer 0.00016113554384066583\n",
            "Accuracy in step 103 is 0.975\n",
            "Average difference between weights for second layer 0.00026872302877575474 and fisrt layer 0.00016053196422812998\n",
            "Accuracy in step 104 is 0.975\n",
            "Average difference between weights for second layer 0.0002670414285669058 and fisrt layer 0.0001598960938431319\n",
            "Accuracy in step 105 is 0.975\n",
            "Average difference between weights for second layer 0.00026539881714357304 and fisrt layer 0.00015924287380419583\n",
            "Accuracy in step 106 is 0.975\n",
            "Average difference between weights for second layer 0.00026379078175846733 and fisrt layer 0.0001585813660138818\n",
            "Accuracy in step 107 is 0.975\n",
            "Average difference between weights for second layer 0.0002622141488188031 and fisrt layer 0.0001579170926186545\n",
            "Accuracy in step 108 is 0.975\n",
            "Average difference between weights for second layer 0.0002606665342373404 and fisrt layer 0.000157253440695995\n",
            "Accuracy in step 109 is 0.975\n",
            "Average difference between weights for second layer 0.0002591460687987021 and fisrt layer 0.00015659250551349713\n",
            "Accuracy in step 110 is 0.975\n",
            "Average difference between weights for second layer 0.0002576512291203253 and fisrt layer 0.00015593559686588972\n",
            "Accuracy in step 111 is 0.975\n",
            "Average difference between weights for second layer 0.00025618073240883676 and fisrt layer 0.0001552835433630935\n",
            "Accuracy in step 112 is 0.975\n",
            "Average difference between weights for second layer 0.00025473346987179353 and fisrt layer 0.0001546368756163214\n",
            "Accuracy in step 113 is 0.975\n",
            "Average difference between weights for second layer 0.00025330846366780244 and fisrt layer 0.00015399593685884395\n",
            "Accuracy in step 114 is 0.975\n",
            "Average difference between weights for second layer 0.00025190594368199726 and fisrt layer 0.00015385679431452996\n",
            "Accuracy in step 115 is 0.975\n",
            "Average difference between weights for second layer 0.00025034758638517836 and fisrt layer 0.00015219104662467613\n",
            "Accuracy in step 116 is 0.975\n",
            "Average difference between weights for second layer 0.0002488461722312031 and fisrt layer 0.00015290583824190314\n",
            "Accuracy in step 117 is 0.975\n",
            "Average difference between weights for second layer 0.0002473780547058425 and fisrt layer 0.00015221361062753609\n",
            "Accuracy in step 118 is 0.975\n",
            "Average difference between weights for second layer 0.0002459278566930544 and fisrt layer 0.00015050762987833864\n",
            "Accuracy in step 119 is 0.975\n",
            "Average difference between weights for second layer 0.00024452044857585075 and fisrt layer 0.00015120187423863058\n",
            "Accuracy in step 120 is 0.975\n",
            "Average difference between weights for second layer 0.00024314143111956845 and fisrt layer 0.00015168705569307807\n",
            "Accuracy in step 121 is 0.975\n",
            "Average difference between weights for second layer 0.00024182249542084588 and fisrt layer 0.00015074950255840113\n",
            "Accuracy in step 122 is 0.975\n",
            "Average difference between weights for second layer 0.00024052028569700493 and fisrt layer 0.00015008016701349044\n",
            "Accuracy in step 123 is 0.975\n",
            "Average difference between weights for second layer 0.0002392315171775694 and fisrt layer 0.00014943191209826915\n",
            "Accuracy in step 124 is 0.975\n",
            "Average difference between weights for second layer 0.0002379580520588267 and fisrt layer 0.00014839378308970457\n",
            "Accuracy in step 125 is 0.975\n",
            "Average difference between weights for second layer 0.0002367007121348408 and fisrt layer 0.0001489554517073581\n",
            "Accuracy in step 126 is 0.975\n",
            "Average difference between weights for second layer 0.00023550364531313382 and fisrt layer 0.0001471386310608432\n",
            "Accuracy in step 127 is 0.975\n",
            "Average difference between weights for second layer 0.00023427425578790456 and fisrt layer 0.00014653405390884893\n",
            "Accuracy in step 128 is 0.975\n",
            "Average difference between weights for second layer 0.00023305946660448889 and fisrt layer 0.00014711405204108523\n",
            "Accuracy in step 129 is 0.975\n",
            "Average difference between weights for second layer 0.00023190400041962512 and fisrt layer 0.00014625241360666555\n",
            "Accuracy in step 130 is 0.975\n",
            "Average difference between weights for second layer 0.00023076223423872395 and fisrt layer 0.0001447062329048224\n",
            "Accuracy in step 131 is 0.975\n",
            "Average difference between weights for second layer 0.00022958544751724042 and fisrt layer 0.00014621664245118165\n",
            "Accuracy in step 132 is 0.975\n",
            "Average difference between weights for second layer 0.00022850978368465808 and fisrt layer 0.0001435001469214324\n",
            "Accuracy in step 133 is 0.975\n",
            "Average difference between weights for second layer 0.00022735503564244036 and fisrt layer 0.00014501261897394062\n",
            "Accuracy in step 134 is 0.975\n",
            "Average difference between weights for second layer 0.00022629926874306113 and fisrt layer 0.0001423207178507611\n",
            "Accuracy in step 135 is 0.975\n",
            "Average difference between weights for second layer 0.0002251661224525652 and fisrt layer 0.0001438409818727322\n",
            "Accuracy in step 136 is 0.975\n",
            "Average difference between weights for second layer 0.00022413005034498432 and fisrt layer 0.00014208294059980505\n",
            "Accuracy in step 137 is 0.975\n",
            "Average difference between weights for second layer 0.00022306070682438645 and fisrt layer 0.00014357549605820944\n",
            "Accuracy in step 138 is 0.975\n",
            "Average difference between weights for second layer 0.00022201435528927405 and fisrt layer 0.00013937343478490774\n",
            "Accuracy in step 139 is 0.975\n",
            "Average difference between weights for second layer 0.00022088663514912736 and fisrt layer 0.0001402683164375738\n",
            "Accuracy in step 140 is 0.975\n",
            "Average difference between weights for second layer 0.00021978588754427853 and fisrt layer 0.00014089853414280118\n",
            "Accuracy in step 141 is 0.975\n",
            "Average difference between weights for second layer 0.00021874396756112186 and fisrt layer 0.00013947591136091102\n",
            "Accuracy in step 142 is 0.975\n",
            "Average difference between weights for second layer 0.00021774551371564466 and fisrt layer 0.00013981329292138653\n",
            "Accuracy in step 143 is 0.975\n",
            "Average difference between weights for second layer 0.00021672222070033388 and fisrt layer 0.00013931082819323\n",
            "Accuracy in step 144 is 0.975\n",
            "Average difference between weights for second layer 0.00021571414337785293 and fisrt layer 0.0001377034868721006\n",
            "Accuracy in step 145 is 0.975\n",
            "Average difference between weights for second layer 0.00021468120766966026 and fisrt layer 0.0001385096620402839\n",
            "Accuracy in step 146 is 0.975\n",
            "Average difference between weights for second layer 0.00021376412470186677 and fisrt layer 0.00013773356923380795\n",
            "Accuracy in step 147 is 0.975\n",
            "Average difference between weights for second layer 0.00021278225892985175 and fisrt layer 0.0001372482382705999\n",
            "Accuracy in step 148 is 0.975\n",
            "Average difference between weights for second layer 0.0002118138470609298 and fisrt layer 0.00013567112166872153\n",
            "Accuracy in step 149 is 0.975\n",
            "Average difference between weights for second layer 0.00021082072839898698 and fisrt layer 0.00013630198127303702\n",
            "Accuracy in step 150 is 0.975\n",
            "Average difference between weights for second layer 0.00020987545533241356 and fisrt layer 0.00013582180944331633\n",
            "Accuracy in step 151 is 0.975\n",
            "Average difference between weights for second layer 0.0002089411827717731 and fisrt layer 0.00013534132425505171\n",
            "Accuracy in step 152 is 0.975\n",
            "Average difference between weights for second layer 0.00020801698432633485 and fisrt layer 0.00013486362241857606\n",
            "Accuracy in step 153 is 0.975\n",
            "Average difference between weights for second layer 0.0002071027771893818 and fisrt layer 0.00013417078413547763\n",
            "Accuracy in step 154 is 0.975\n",
            "Average difference between weights for second layer 0.0002062005328373072 and fisrt layer 0.00013391639142372084\n",
            "Accuracy in step 155 is 0.975\n",
            "Average difference between weights for second layer 0.0002053031739022455 and fisrt layer 0.00013534314689465706\n",
            "Accuracy in step 156 is 0.975\n",
            "Average difference between weights for second layer 0.00020445738783285368 and fisrt layer 0.00013374525122502885\n",
            "Accuracy in step 157 is 0.975\n",
            "Average difference between weights for second layer 0.0002035544000910302 and fisrt layer 0.000134000975888044\n",
            "Accuracy in step 158 is 0.975\n",
            "Average difference between weights for second layer 0.00020268217069485165 and fisrt layer 0.00013258383788916184\n",
            "Accuracy in step 159 is 0.975\n",
            "Average difference between weights for second layer 0.00020177345729938795 and fisrt layer 0.00013294281968549487\n",
            "Accuracy in step 160 is 0.975\n",
            "Average difference between weights for second layer 0.0002009054637451338 and fisrt layer 0.00013263617291539248\n",
            "Accuracy in step 161 is 0.975\n",
            "Average difference between weights for second layer 0.00020010680716056237 and fisrt layer 0.00012996763815804606\n",
            "Accuracy in step 162 is 0.975\n",
            "Average difference between weights for second layer 0.0001991739179548528 and fisrt layer 0.00013253456709248383\n",
            "Accuracy in step 163 is 0.975\n",
            "Average difference between weights for second layer 0.00019832969189698945 and fisrt layer 0.00013078714462594923\n",
            "Accuracy in step 164 is 0.975\n",
            "Average difference between weights for second layer 0.00019745798461034622 and fisrt layer 0.00013045853333790435\n",
            "Accuracy in step 165 is 0.975\n",
            "Average difference between weights for second layer 0.00019660963704183935 and fisrt layer 0.00013027508289635215\n",
            "Accuracy in step 166 is 0.975\n",
            "Average difference between weights for second layer 0.00019575362366195003 and fisrt layer 0.0001294414763602367\n",
            "Accuracy in step 167 is 0.975\n",
            "Average difference between weights for second layer 0.00019490551460744508 and fisrt layer 0.0001291474920632564\n",
            "Accuracy in step 168 is 0.975\n",
            "Average difference between weights for second layer 0.00019408248953769577 and fisrt layer 0.00012982737167053138\n",
            "Accuracy in step 169 is 0.975\n",
            "Average difference between weights for second layer 0.00019328765554655123 and fisrt layer 0.00012769780849818864\n",
            "Accuracy in step 170 is 0.975\n",
            "Average difference between weights for second layer 0.00019246405985881476 and fisrt layer 0.00012845329648299476\n",
            "Accuracy in step 171 is 0.975\n",
            "Average difference between weights for second layer 0.0001916767163127425 and fisrt layer 0.0001268130377556067\n",
            "Accuracy in step 172 is 0.975\n",
            "Average difference between weights for second layer 0.0001908637195883334 and fisrt layer 0.000127598586519475\n",
            "Accuracy in step 173 is 0.975\n",
            "Average difference between weights for second layer 0.00019009005905102236 and fisrt layer 0.00012701022601730694\n",
            "Accuracy in step 174 is 0.975\n",
            "Average difference between weights for second layer 0.00018930425799226576 and fisrt layer 0.00012548160787951814\n",
            "Accuracy in step 175 is 0.975\n",
            "Average difference between weights for second layer 0.0001885021382158824 and fisrt layer 0.00012633183091206212\n",
            "Accuracy in step 176 is 0.975\n",
            "Average difference between weights for second layer 0.0001877465622022911 and fisrt layer 0.00012476143236315546\n",
            "Accuracy in step 177 is 0.975\n",
            "Average difference between weights for second layer 0.00018696744175275356 and fisrt layer 0.00012662137377619586\n",
            "Accuracy in step 178 is 0.975\n",
            "Average difference between weights for second layer 0.0001862471526906802 and fisrt layer 0.00012373443556395523\n",
            "Accuracy in step 179 is 0.975\n",
            "Average difference between weights for second layer 0.00018545381515852594 and fisrt layer 0.00012468880790243196\n",
            "Accuracy in step 180 is 0.975\n",
            "Average difference between weights for second layer 0.0001847190928812281 and fisrt layer 0.00012404069280380915\n",
            "Accuracy in step 181 is 0.975\n",
            "Average difference between weights for second layer 0.00018399395280998564 and fisrt layer 0.00012505106227081315\n",
            "Accuracy in step 182 is 0.975\n",
            "Average difference between weights for second layer 0.00018330238837670524 and fisrt layer 0.0001221958373296797\n",
            "Accuracy in step 183 is 0.975\n",
            "Average difference between weights for second layer 0.00018253537217768852 and fisrt layer 0.00012401549782390628\n",
            "Accuracy in step 184 is 0.975\n",
            "Average difference between weights for second layer 0.00018185701931871143 and fisrt layer 0.00012267865662433873\n",
            "Accuracy in step 185 is 0.975\n",
            "Average difference between weights for second layer 0.00018114450745765755 and fisrt layer 0.0001220890192683784\n",
            "Accuracy in step 186 is 0.975\n",
            "Average difference between weights for second layer 0.00018044398892625173 and fisrt layer 0.00012276155669974136\n",
            "Accuracy in step 187 is 0.975\n",
            "Average difference between weights for second layer 0.00017968999795216483 and fisrt layer 0.00012139523060707433\n",
            "Accuracy in step 188 is 0.975\n",
            "Average difference between weights for second layer 0.000179006251827703 and fisrt layer 0.00012207074053818096\n",
            "Accuracy in step 189 is 0.975\n",
            "Average difference between weights for second layer 0.00017832150783772598 and fisrt layer 0.00012070523329793466\n",
            "Accuracy in step 190 is 0.975\n",
            "Average difference between weights for second layer 0.00017760277219983143 and fisrt layer 0.000122784305343855\n",
            "Accuracy in step 191 is 0.975\n",
            "Average difference between weights for second layer 0.000177191943990464 and fisrt layer 0.00011938549447655352\n",
            "Accuracy in step 192 is 0.975\n",
            "Average difference between weights for second layer 0.00017622953999333062 and fisrt layer 0.00012198019567182222\n",
            "Accuracy in step 193 is 0.975\n",
            "Average difference between weights for second layer 0.00017589113581127888 and fisrt layer 0.00011890950360259869\n",
            "Accuracy in step 194 is 0.975\n",
            "Average difference between weights for second layer 0.00017494230465931726 and fisrt layer 0.00012133373362001235\n",
            "Accuracy in step 195 is 0.975\n",
            "Average difference between weights for second layer 0.00017463745810336178 and fisrt layer 0.00011805158226470218\n",
            "Accuracy in step 196 is 0.975\n",
            "Average difference between weights for second layer 0.00017370650462800532 and fisrt layer 0.00012067332582647931\n",
            "Accuracy in step 197 is 0.975\n",
            "Average difference between weights for second layer 0.00017341444856715542 and fisrt layer 0.00011795065480702884\n",
            "Accuracy in step 198 is 0.975\n",
            "Average difference between weights for second layer 0.00017266076544420204 and fisrt layer 0.00011938360969965557\n",
            "Accuracy in step 199 is 0.975\n",
            "Average difference between weights for second layer 0.00017215102123884095 and fisrt layer 0.00011719983005145015\n",
            "Accuracy in step 200 is 0.975\n",
            "Average difference between weights for second layer 0.0001714366250321396 and fisrt layer 0.00011979531661177202\n",
            "Accuracy in step 201 is 0.975\n",
            "Average difference between weights for second layer 0.00017096582749402903 and fisrt layer 0.00011657369747171623\n",
            "Accuracy in step 202 is 0.975\n",
            "Average difference between weights for second layer 0.00017027226980604365 and fisrt layer 0.0001191651216329259\n",
            "Accuracy in step 203 is 0.975\n",
            "Average difference between weights for second layer 0.00016981956353603441 and fisrt layer 0.00011694314640695757\n",
            "Accuracy in step 204 is 0.975\n",
            "Average difference between weights for second layer 0.00016929602038942153 and fisrt layer 0.0001171826192836082\n",
            "Accuracy in step 205 is 0.975\n",
            "Average difference between weights for second layer 0.00016875087694688914 and fisrt layer 0.00011611221004769186\n",
            "Accuracy in step 206 is 0.975\n",
            "Average difference between weights for second layer 0.00016803582722077216 and fisrt layer 0.00011721127781701732\n",
            "Accuracy in step 207 is 0.975\n",
            "Average difference between weights for second layer 0.00016773436127299015 and fisrt layer 0.00011499540533951078\n",
            "Accuracy in step 208 is 0.975\n",
            "Average difference between weights for second layer 0.00016683549663336355 and fisrt layer 0.00011679186554965364\n",
            "Accuracy in step 209 is 0.975\n",
            "Average difference between weights for second layer 0.0001665915346569662 and fisrt layer 0.0001164771767827684\n",
            "Accuracy in step 210 is 0.975\n",
            "Average difference between weights for second layer 0.00016604434911340812 and fisrt layer 0.00011353797033421242\n",
            "Accuracy in step 211 is 0.975\n",
            "Average difference between weights for second layer 0.00016538905494126435 and fisrt layer 0.00011625208482683334\n",
            "Accuracy in step 212 is 0.975\n",
            "Average difference between weights for second layer 0.00016497226557366545 and fisrt layer 0.0001141427431836915\n",
            "Accuracy in step 213 is 0.975\n",
            "Average difference between weights for second layer 0.00016450134789686288 and fisrt layer 0.00011578338615326264\n",
            "Accuracy in step 214 is 0.975\n",
            "Average difference between weights for second layer 0.00016418567085439465 and fisrt layer 0.00011283622428946289\n",
            "Accuracy in step 215 is 0.975\n",
            "Average difference between weights for second layer 0.0001634887971420301 and fisrt layer 0.00011426677477795128\n",
            "Accuracy in step 216 is 0.975\n",
            "Average difference between weights for second layer 0.00016305946522850113 and fisrt layer 0.00011360267898485935\n",
            "Accuracy in step 217 is 0.975\n",
            "Average difference between weights for second layer 0.00016258121044239685 and fisrt layer 0.00011430638202355507\n",
            "Accuracy in step 218 is 0.975\n",
            "Average difference between weights for second layer 0.0001621520228118264 and fisrt layer 0.00011295369191556594\n",
            "Accuracy in step 219 is 0.975\n",
            "Average difference between weights for second layer 0.00016166590687353213 and fisrt layer 0.00011294595315707796\n",
            "Accuracy in step 220 is 0.975\n",
            "Average difference between weights for second layer 0.0001612066763433725 and fisrt layer 0.00011347297963533181\n",
            "Accuracy in step 221 is 0.975\n",
            "Average difference between weights for second layer 0.0001607926688246719 and fisrt layer 0.00011211933063426606\n",
            "Accuracy in step 222 is 0.975\n",
            "Average difference between weights for second layer 0.0001603187861187624 and fisrt layer 0.00011285330069579015\n",
            "Accuracy in step 223 is 0.975\n",
            "Average difference between weights for second layer 0.0001598941397323166 and fisrt layer 0.00011153240602534226\n",
            "Accuracy in step 224 is 0.975\n",
            "Average difference between weights for second layer 0.0001594192787497325 and fisrt layer 0.00011204394903070386\n",
            "Accuracy in step 225 is 0.975\n",
            "Average difference between weights for second layer 0.000158918340923164 and fisrt layer 0.00011177339308181164\n",
            "Accuracy in step 226 is 0.975\n",
            "Average difference between weights for second layer 0.00015841969669487389 and fisrt layer 0.00011066183624423833\n",
            "Accuracy in step 227 is 0.975\n",
            "Average difference between weights for second layer 0.00015792872923913161 and fisrt layer 0.00011130915713380637\n",
            "Accuracy in step 228 is 0.975\n",
            "Average difference between weights for second layer 0.00015745196248163207 and fisrt layer 0.000110211811939663\n",
            "Accuracy in step 229 is 0.975\n",
            "Average difference between weights for second layer 0.00015697852995121917 and fisrt layer 0.00010987439352422402\n",
            "Accuracy in step 230 is 0.975\n",
            "Average difference between weights for second layer 0.00015638537311537975 and fisrt layer 0.0001127383310886825\n",
            "Accuracy in step 231 is 0.975\n",
            "Average difference between weights for second layer 0.0001561842221275212 and fisrt layer 0.00010829987362187127\n",
            "Accuracy in step 232 is 0.975\n",
            "Average difference between weights for second layer 0.00015554196009596915 and fisrt layer 0.00011066597709479502\n",
            "Accuracy in step 233 is 0.975\n",
            "Average difference between weights for second layer 0.0001552782846517629 and fisrt layer 0.0001102122396402627\n",
            "Accuracy in step 234 is 0.975\n",
            "Average difference between weights for second layer 0.0001547054930254259 and fisrt layer 0.00011014304634315484\n",
            "Accuracy in step 235 is 0.975\n",
            "Average difference between weights for second layer 0.00015444871304790548 and fisrt layer 0.00010899635975893125\n",
            "Accuracy in step 236 is 0.975\n",
            "Average difference between weights for second layer 0.00015396282422971687 and fisrt layer 0.00010840398315498807\n",
            "Accuracy in step 237 is 0.975\n",
            "Average difference between weights for second layer 0.00015349691338357698 and fisrt layer 0.00011030844455778916\n",
            "Accuracy in step 238 is 0.975\n",
            "Average difference between weights for second layer 0.00015310381034529626 and fisrt layer 0.00010706261104989967\n",
            "Accuracy in step 239 is 0.975\n",
            "Average difference between weights for second layer 0.00015264131154186895 and fisrt layer 0.00010986060891728234\n",
            "Accuracy in step 240 is 0.975\n",
            "Average difference between weights for second layer 0.00015226364244630446 and fisrt layer 0.00010848083763465344\n",
            "Accuracy in step 241 is 0.975\n",
            "Average difference between weights for second layer 0.0001519696803982173 and fisrt layer 0.00010673720525575241\n",
            "Accuracy in step 242 is 0.975\n",
            "Average difference between weights for second layer 0.00015134496747186025 and fisrt layer 0.00010869314208757753\n",
            "Accuracy in step 243 is 0.975\n",
            "Average difference between weights for second layer 0.00015111489350808985 and fisrt layer 0.00010635083785648987\n",
            "Accuracy in step 244 is 0.975\n",
            "Average difference between weights for second layer 0.00015058342329035609 and fisrt layer 0.0001091729117810782\n",
            "Accuracy in step 245 is 0.975\n",
            "Average difference between weights for second layer 0.00015032744199484067 and fisrt layer 0.00010684964610184398\n",
            "Accuracy in step 246 is 0.975\n",
            "Average difference between weights for second layer 0.00014987618519098143 and fisrt layer 0.00010660512722415126\n",
            "Accuracy in step 247 is 0.975\n",
            "Average difference between weights for second layer 0.00014945937620905298 and fisrt layer 0.00010652887314630471\n",
            "Accuracy in step 248 is 0.975\n",
            "Average difference between weights for second layer 0.00014905024046037477 and fisrt layer 0.00010806772108723859\n",
            "Accuracy in step 249 is 0.975\n",
            "Average difference between weights for second layer 0.00014881532535726292 and fisrt layer 0.0001058532976386896\n",
            "Accuracy in step 250 is 0.975\n",
            "Average difference between weights for second layer 0.00014834607156215477 and fisrt layer 0.00010654777631458191\n",
            "Accuracy in step 251 is 0.975\n",
            "Average difference between weights for second layer 0.00014800696678503195 and fisrt layer 0.00010470728685317504\n",
            "Accuracy in step 252 is 0.975\n",
            "Average difference between weights for second layer 0.00014755328999560612 and fisrt layer 0.00010715400733022394\n",
            "Accuracy in step 253 is 0.975\n",
            "Average difference between weights for second layer 0.00014728763628358912 and fisrt layer 0.00010474112403254098\n",
            "Accuracy in step 254 is 0.975\n",
            "Average difference between weights for second layer 0.00014694461420465554 and fisrt layer 0.00010615343182248612\n",
            "Accuracy in step 255 is 0.975\n",
            "Average difference between weights for second layer 0.0001465226904478259 and fisrt layer 0.00010531249891257899\n",
            "Accuracy in step 256 is 0.975\n",
            "Average difference between weights for second layer 0.00014616567090307083 and fisrt layer 0.00010549495233498717\n",
            "Accuracy in step 257 is 0.975\n",
            "Average difference between weights for second layer 0.0001458426499908325 and fisrt layer 0.0001038880248067095\n",
            "Accuracy in step 258 is 0.975\n",
            "Average difference between weights for second layer 0.0001454093245229391 and fisrt layer 0.0001040916247781709\n",
            "Accuracy in step 259 is 0.975\n",
            "Average difference between weights for second layer 0.00014514648692210653 and fisrt layer 0.00010458858522996148\n",
            "Accuracy in step 260 is 0.975\n",
            "Average difference between weights for second layer 0.0001447824034550093 and fisrt layer 0.00010436677423355248\n",
            "Accuracy in step 261 is 0.975\n",
            "Average difference between weights for second layer 0.0001444234768927871 and fisrt layer 0.00010518226635622565\n",
            "Accuracy in step 262 is 0.975\n",
            "Average difference between weights for second layer 0.00014407108448557488 and fisrt layer 0.00010374519505987438\n",
            "Accuracy in step 263 is 0.975\n",
            "Average difference between weights for second layer 0.00014373953088504463 and fisrt layer 0.00010304484199941612\n",
            "Accuracy in step 264 is 0.975\n",
            "Average difference between weights for second layer 0.0001434789650619313 and fisrt layer 0.00010353464839318464\n",
            "Accuracy in step 265 is 0.975\n",
            "Average difference between weights for second layer 0.0001431197487430533 and fisrt layer 0.00010280442897221996\n",
            "Accuracy in step 266 is 0.975\n",
            "Average difference between weights for second layer 0.0001427061314452214 and fisrt layer 0.00010360728703152417\n",
            "Accuracy in step 267 is 0.975\n",
            "Average difference between weights for second layer 0.00014241459321614634 and fisrt layer 0.0001031626803352854\n",
            "Accuracy in step 268 is 0.975\n",
            "Average difference between weights for second layer 0.00014211399040211248 and fisrt layer 0.00010247335338682048\n",
            "Accuracy in step 269 is 0.975\n",
            "Average difference between weights for second layer 0.00014177613124019341 and fisrt layer 0.00010268858209433365\n",
            "Accuracy in step 270 is 0.975\n",
            "Average difference between weights for second layer 0.0001415171417583137 and fisrt layer 0.00010176145982396486\n",
            "Accuracy in step 271 is 0.975\n",
            "Average difference between weights for second layer 0.00014122081249239726 and fisrt layer 0.00010253597750591571\n",
            "Accuracy in step 272 is 0.975\n",
            "Average difference between weights for second layer 0.0001409190352819351 and fisrt layer 0.0001017432769540858\n",
            "Accuracy in step 273 is 0.975\n",
            "Average difference between weights for second layer 0.00014060253345440606 and fisrt layer 0.00010260478868631063\n",
            "Accuracy in step 274 is 0.975\n",
            "Average difference between weights for second layer 0.00014037479574345585 and fisrt layer 0.00010124223317366544\n",
            "Accuracy in step 275 is 0.975\n",
            "Average difference between weights for second layer 0.00014004754041168873 and fisrt layer 0.00010115080217297532\n",
            "Accuracy in step 276 is 0.975\n",
            "Average difference between weights for second layer 0.0001397348627776802 and fisrt layer 0.00010201304659871837\n",
            "Accuracy in step 277 is 0.975\n",
            "Average difference between weights for second layer 0.00013950983569664555 and fisrt layer 0.00010066390950409932\n",
            "Accuracy in step 278 is 0.975\n",
            "Average difference between weights for second layer 0.0001391871549088747 and fisrt layer 0.0001015681632424438\n",
            "Accuracy in step 279 is 0.975\n",
            "Average difference between weights for second layer 0.00013895789677042595 and fisrt layer 9.955913105077333e-05\n",
            "Accuracy in step 280 is 0.975\n",
            "Average difference between weights for second layer 0.0001386128652427598 and fisrt layer 0.00010052553235475593\n",
            "Accuracy in step 281 is 0.975\n",
            "Average difference between weights for second layer 0.0001383661574116726 and fisrt layer 0.00010052448495371541\n",
            "Accuracy in step 282 is 0.975\n",
            "Average difference between weights for second layer 0.00013806076075257881 and fisrt layer 0.00010015252318732072\n",
            "Accuracy in step 283 is 0.975\n",
            "Average difference between weights for second layer 0.00013781774468904208 and fisrt layer 0.00010014508117986507\n",
            "Accuracy in step 284 is 0.975\n",
            "Average difference between weights for second layer 0.00013751548607993306 and fisrt layer 9.977515333248378e-05\n",
            "Accuracy in step 285 is 0.975\n",
            "Average difference between weights for second layer 0.00013727479718287687 and fisrt layer 9.97351889359673e-05\n",
            "Accuracy in step 286 is 0.975\n",
            "Average difference between weights for second layer 0.0001369739231502666 and fisrt layer 9.937342483200716e-05\n",
            "Accuracy in step 287 is 0.975\n",
            "Average difference between weights for second layer 0.0001367346468715396 and fisrt layer 9.936689357376946e-05\n",
            "Accuracy in step 288 is 0.975\n",
            "Average difference between weights for second layer 0.0001364366251800945 and fisrt layer 9.900780804829353e-05\n",
            "Accuracy in step 289 is 0.975\n",
            "Average difference between weights for second layer 0.00013619968150807544 and fisrt layer 9.900033006566742e-05\n",
            "Accuracy in step 290 is 0.975\n",
            "Average difference between weights for second layer 0.0001359039799309951 and fisrt layer 9.86456170261153e-05\n",
            "Accuracy in step 291 is 0.975\n",
            "Average difference between weights for second layer 0.00013566916650664717 and fisrt layer 9.863803354957619e-05\n",
            "Accuracy in step 292 is 0.975\n",
            "Average difference between weights for second layer 0.00013537574713587638 and fisrt layer 9.856870579286227e-05\n",
            "Accuracy in step 293 is 0.975\n",
            "Average difference between weights for second layer 0.00013509940648949315 and fisrt layer 9.79207700392516e-05\n",
            "Accuracy in step 294 is 0.975\n",
            "Average difference between weights for second layer 0.00013483319314386416 and fisrt layer 9.798635611471236e-05\n",
            "Accuracy in step 295 is 0.975\n",
            "Average difference between weights for second layer 0.00013451755998236003 and fisrt layer 9.768850657199274e-05\n",
            "Accuracy in step 296 is 0.975\n",
            "Average difference between weights for second layer 0.0001342662006998484 and fisrt layer 9.795511358623629e-05\n",
            "Accuracy in step 297 is 0.975\n",
            "Average difference between weights for second layer 0.00013400155301347352 and fisrt layer 9.79103940068718e-05\n",
            "Accuracy in step 298 is 0.975\n",
            "Average difference between weights for second layer 0.00013376050563755737 and fisrt layer 9.67227165462787e-05\n",
            "Accuracy in step 299 is 0.975\n",
            "Average difference between weights for second layer 0.00013343828773031792 and fisrt layer 9.766170117126856e-05\n",
            "Accuracy in step 300 is 0.975\n",
            "Average difference between weights for second layer 0.0001332091274052079 and fisrt layer 9.705619415527886e-05\n",
            "Accuracy in step 301 is 0.975\n",
            "Average difference between weights for second layer 0.00013291056968949577 and fisrt layer 9.66181557798027e-05\n",
            "Accuracy in step 302 is 0.975\n",
            "Average difference between weights for second layer 0.00013265422572328946 and fisrt layer 9.713709469933965e-05\n",
            "Accuracy in step 303 is 0.975\n",
            "Average difference between weights for second layer 0.0001324275380805776 and fisrt layer 9.65410244493952e-05\n",
            "Accuracy in step 304 is 0.975\n",
            "Average difference between weights for second layer 0.00013213285756846505 and fisrt layer 9.623709202393813e-05\n",
            "Accuracy in step 305 is 0.975\n",
            "Average difference between weights for second layer 0.00013189804553301096 and fisrt layer 9.624431712202805e-05\n",
            "Accuracy in step 306 is 0.975\n",
            "Average difference between weights for second layer 0.0001316080201679532 and fisrt layer 9.620385547373092e-05\n",
            "Accuracy in step 307 is 0.975\n",
            "Average difference between weights for second layer 0.0001313349451255891 and fisrt layer 9.643896091696521e-05\n",
            "Accuracy in step 308 is 0.975\n",
            "Average difference between weights for second layer 0.00013112798624460864 and fisrt layer 9.520448689756863e-05\n",
            "Accuracy in step 309 is 0.975\n",
            "Average difference between weights for second layer 0.00013083254399311243 and fisrt layer 9.575629442776428e-05\n",
            "Accuracy in step 310 is 0.975\n",
            "Average difference between weights for second layer 0.00013056597765765372 and fisrt layer 9.598508163730237e-05\n",
            "Accuracy in step 311 is 0.975\n",
            "Average difference between weights for second layer 0.00013036382119264658 and fisrt layer 9.475511130204654e-05\n",
            "Accuracy in step 312 is 0.975\n",
            "Average difference between weights for second layer 0.00013007392441159485 and fisrt layer 9.530138793360129e-05\n",
            "Accuracy in step 313 is 0.975\n",
            "Average difference between weights for second layer 0.00012981228402165367 and fisrt layer 9.552870839432121e-05\n",
            "Accuracy in step 314 is 0.975\n",
            "Average difference between weights for second layer 0.00012961418450458854 and fisrt layer 9.482992876150521e-05\n",
            "Accuracy in step 315 is 0.975\n",
            "Average difference between weights for second layer 0.00012934174764392238 and fisrt layer 9.510791799911737e-05\n",
            "Accuracy in step 316 is 0.975\n",
            "Average difference between weights for second layer 0.00012913917997273502 and fisrt layer 9.391526891325765e-05\n",
            "Accuracy in step 317 is 0.975\n",
            "Average difference between weights for second layer 0.0001288527523403443 and fisrt layer 9.446832614979533e-05\n",
            "Accuracy in step 318 is 0.975\n",
            "Average difference between weights for second layer 0.00012859562667506214 and fisrt layer 9.47029160185945e-05\n",
            "Accuracy in step 319 is 0.975\n",
            "Average difference between weights for second layer 0.0001284020096775869 and fisrt layer 9.407448712360318e-05\n",
            "Accuracy in step 320 is 0.975\n",
            "Average difference between weights for second layer 0.00012813706422257678 and fisrt layer 9.402022016628723e-05\n",
            "Accuracy in step 321 is 0.975\n",
            "Average difference between weights for second layer 0.000127885806932865 and fisrt layer 9.357786979550896e-05\n",
            "Accuracy in step 322 is 0.975\n",
            "Average difference between weights for second layer 0.0001276789235708416 and fisrt layer 9.354843442518337e-05\n",
            "Accuracy in step 323 is 0.975\n",
            "Average difference between weights for second layer 0.0001274160013139264 and fisrt layer 9.345191959695074e-05\n",
            "Accuracy in step 324 is 0.975\n",
            "Average difference between weights for second layer 0.0001271657497979069 and fisrt layer 9.368714291535959e-05\n",
            "Accuracy in step 325 is 0.975\n",
            "Average difference between weights for second layer 0.00012697760920959647 and fisrt layer 9.30661695103654e-05\n",
            "Accuracy in step 326 is 0.975\n",
            "Average difference between weights for second layer 0.00012671931723407933 and fisrt layer 9.244846888895923e-05\n",
            "Accuracy in step 327 is 0.975\n",
            "Average difference between weights for second layer 0.00012646037694361747 and fisrt layer 9.325686259790405e-05\n",
            "Accuracy in step 328 is 0.975\n",
            "Average difference between weights for second layer 0.0001262755456840803 and fisrt layer 9.263837462536087e-05\n",
            "Accuracy in step 329 is 0.975\n",
            "Average difference between weights for second layer 0.00012602112721349736 and fisrt layer 9.258913826801367e-05\n",
            "Accuracy in step 330 is 0.975\n",
            "Average difference between weights for second layer 0.00012577979068998545 and fisrt layer 9.249180451806761e-05\n",
            "Accuracy in step 331 is 0.975\n",
            "Average difference between weights for second layer 0.00012554562964609402 and fisrt layer 9.264699295392204e-05\n",
            "Accuracy in step 332 is 0.975\n",
            "Average difference between weights for second layer 0.0001253663516702387 and fisrt layer 9.202268572064142e-05\n",
            "Accuracy in step 333 is 0.975\n",
            "Average difference between weights for second layer 0.00012511709561209553 and fisrt layer 9.197044070325e-05\n",
            "Accuracy in step 334 is 0.975\n",
            "Average difference between weights for second layer 0.0001248801761966421 and fisrt layer 9.219462397896255e-05\n",
            "Accuracy in step 335 is 0.975\n",
            "Average difference between weights for second layer 0.0001247017902715136 and fisrt layer 9.098426872804375e-05\n",
            "Accuracy in step 336 is 0.975\n",
            "Average difference between weights for second layer 0.00012443989159958518 and fisrt layer 9.147482353236673e-05\n",
            "Accuracy in step 337 is 0.975\n",
            "Average difference between weights for second layer 0.0001242033935620388 and fisrt layer 9.24278918602201e-05\n",
            "Accuracy in step 338 is 0.975\n",
            "Average difference between weights for second layer 0.000124043636216086 and fisrt layer 9.108983477320951e-05\n",
            "Accuracy in step 339 is 0.975\n",
            "Average difference between weights for second layer 0.00012379949431219386 and fisrt layer 9.10415388185864e-05\n",
            "Accuracy in step 340 is 0.975\n",
            "Average difference between weights for second layer 0.00012356738012964997 and fisrt layer 9.126739887217085e-05\n",
            "Accuracy in step 341 is 0.975\n",
            "Average difference between weights for second layer 0.00012339281483643644 and fisrt layer 9.065910511749015e-05\n",
            "Accuracy in step 342 is 0.975\n",
            "Average difference between weights for second layer 0.00012315067736761413 and fisrt layer 9.144202688961264e-05\n",
            "Accuracy in step 343 is 0.975\n",
            "Average difference between weights for second layer 0.0001229749523399391 and fisrt layer 9.024900664264284e-05\n",
            "Accuracy in step 344 is 0.975\n",
            "Average difference between weights for second layer 0.00012271834798129212 and fisrt layer 9.05956205227323e-05\n",
            "Accuracy in step 345 is 0.975\n",
            "Average difference between weights for second layer 0.0001225328676675194 and fisrt layer 9.005078472104657e-05\n",
            "Accuracy in step 346 is 0.975\n",
            "Average difference between weights for second layer 0.00012228793796615557 and fisrt layer 9.003768259088752e-05\n",
            "Accuracy in step 347 is 0.975\n",
            "Average difference between weights for second layer 0.00012205877549432706 and fisrt layer 9.027887642021131e-05\n",
            "Accuracy in step 348 is 0.975\n",
            "Average difference between weights for second layer 0.0001218875356533156 and fisrt layer 9.050279905721782e-05\n",
            "Accuracy in step 349 is 0.975\n",
            "Average difference between weights for second layer 0.00012170440621351004 and fisrt layer 8.937302975216841e-05\n",
            "Accuracy in step 350 is 0.975\n",
            "Average difference between weights for second layer 0.00012144765293045533 and fisrt layer 8.974456162454251e-05\n",
            "Accuracy in step 351 is 0.975\n",
            "Average difference between weights for second layer 0.00012126377892040927 and fisrt layer 8.921737694472471e-05\n",
            "Accuracy in step 352 is 0.975\n",
            "Average difference between weights for second layer 0.00012102292689930283 and fisrt layer 8.921254227886526e-05\n",
            "Accuracy in step 353 is 0.975\n",
            "Average difference between weights for second layer 0.00012079828798750796 and fisrt layer 9.026656180160138e-05\n",
            "Accuracy in step 354 is 0.975\n",
            "Average difference between weights for second layer 0.00012068334405523578 and fisrt layer 8.859522190087967e-05\n",
            "Accuracy in step 355 is 0.975\n",
            "Average difference between weights for second layer 0.00012041945351920112 and fisrt layer 8.870376559300084e-05\n",
            "Accuracy in step 356 is 0.975\n",
            "Average difference between weights for second layer 0.00012018484219059114 and fisrt layer 8.900218890804677e-05\n",
            "Accuracy in step 357 is 0.975\n",
            "Average difference between weights for second layer 0.0001200132567933334 and fisrt layer 8.839684021598344e-05\n",
            "Accuracy in step 358 is 0.975\n",
            "Average difference between weights for second layer 0.0001197801752930044 and fisrt layer 8.918522847478019e-05\n",
            "Accuracy in step 359 is 0.975\n",
            "Average difference between weights for second layer 0.00011961342473694079 and fisrt layer 8.834694251608102e-05\n",
            "Accuracy in step 360 is 0.975\n",
            "Average difference between weights for second layer 0.00011941804989711335 and fisrt layer 8.790561762917567e-05\n",
            "Accuracy in step 361 is 0.975\n",
            "Average difference between weights for second layer 0.00011917625730867317 and fisrt layer 8.7942176252725e-05\n",
            "Accuracy in step 362 is 0.975\n",
            "Average difference between weights for second layer 0.00011895458003478884 and fisrt layer 8.820427904961754e-05\n",
            "Accuracy in step 363 is 0.975\n",
            "Average difference between weights for second layer 0.00011879066251998936 and fisrt layer 8.837879106183485e-05\n",
            "Accuracy in step 364 is 0.975\n",
            "Average difference between weights for second layer 0.00011861526442627116 and fisrt layer 8.729039195840331e-05\n",
            "Accuracy in step 365 is 0.975\n",
            "Average difference between weights for second layer 0.00011837019333009903 and fisrt layer 8.766710717641381e-05\n",
            "Accuracy in step 366 is 0.975\n",
            "Average difference between weights for second layer 0.00011819554142190754 and fisrt layer 8.715648013546881e-05\n",
            "Accuracy in step 367 is 0.975\n",
            "Average difference between weights for second layer 0.00011796668455973109 and fisrt layer 8.711352119428196e-05\n",
            "Accuracy in step 368 is 0.975\n",
            "Average difference between weights for second layer 0.00011775215645609653 and fisrt layer 8.78439898906128e-05\n",
            "Accuracy in step 369 is 0.975\n",
            "Average difference between weights for second layer 0.00011759671462090157 and fisrt layer 8.699791229562313e-05\n",
            "Accuracy in step 370 is 0.975\n",
            "Average difference between weights for second layer 0.00011741024942437718 and fisrt layer 8.655537052415543e-05\n",
            "Accuracy in step 371 is 0.975\n",
            "Average difference between weights for second layer 0.00011717775649835247 and fisrt layer 8.658995794002791e-05\n",
            "Accuracy in step 372 is 0.975\n",
            "Average difference between weights for second layer 0.00011696426212159079 and fisrt layer 8.735846461721639e-05\n",
            "Accuracy in step 373 is 0.975\n",
            "Average difference between weights for second layer 0.00011681618121409313 and fisrt layer 8.529894516235326e-05\n",
            "Accuracy in step 374 is 0.975\n",
            "Average difference between weights for second layer 0.0001165319082308 and fisrt layer 8.724724492358654e-05\n",
            "Accuracy in step 375 is 0.975\n",
            "Average difference between weights for second layer 0.0001163981178826164 and fisrt layer 8.491400816865383e-05\n",
            "Accuracy in step 376 is 0.975\n",
            "Average difference between weights for second layer 0.00011609096621688262 and fisrt layer 8.752710736480836e-05\n",
            "Accuracy in step 377 is 0.975\n",
            "Average difference between weights for second layer 0.00011603897884710008 and fisrt layer 8.451856276891531e-05\n",
            "Accuracy in step 378 is 0.975\n",
            "Average difference between weights for second layer 0.00011571985371901277 and fisrt layer 8.692457572006567e-05\n",
            "Accuracy in step 379 is 0.975\n",
            "Average difference between weights for second layer 0.00011561764775831629 and fisrt layer 8.601387926770262e-05\n",
            "Accuracy in step 380 is 0.975\n",
            "Average difference between weights for second layer 0.00011541271937850234 and fisrt layer 8.490378172928712e-05\n",
            "Accuracy in step 381 is 0.975\n",
            "Average difference between weights for second layer 0.00011520684140637575 and fisrt layer 8.599764433788988e-05\n",
            "Accuracy in step 382 is 0.975\n",
            "Average difference between weights for second layer 0.00011503661050556612 and fisrt layer 8.411174382101335e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_nn_test(X_test, y_test_one_hot, w1, w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySHwOIlIujo1",
        "outputId": "0b31bc02-f823-4655-f90b-99c03ff8bf3d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy in test set is 100.0%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 1, 3, 3, 1, 1, 2, 2, 3, 1, 3, 1, 1, 2, 2, 3, 3, 3, 2, 2, 3, 3,\n",
              "       1, 3, 1, 1, 1, 1, 1, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DLpMa-0Za4Cv"
      },
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}